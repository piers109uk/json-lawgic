{"endpoint_name":null,"id":"3b91dd1c-fbbd-4a86-94ff-56383218f9cc","tags":null,"folder_id":"96bfc064-14e0-4565-ae84-e9a405d3b52d","gradient":null,"data":{"nodes":[{"id":"URL-lCw7J","type":"genericNode","position":{"x":-210.19347116991523,"y":-79.3465022859368},"data":{"type":"URL","node":{"template":{"_type":"Component","code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"import re\n\nfrom langchain_community.document_loaders import AsyncHtmlLoader, WebBaseLoader\n\nfrom langflow.custom import Component\nfrom langflow.helpers.data import data_to_text\nfrom langflow.io import DropdownInput, MessageTextInput, Output\nfrom langflow.schema import Data\nfrom langflow.schema.message import Message\n\n\nclass URLComponent(Component):\n    display_name = \"URL\"\n    description = \"Fetch content from one or more URLs.\"\n    icon = \"layout-template\"\n    name = \"URL\"\n\n    inputs = [\n        MessageTextInput(\n            name=\"urls\",\n            display_name=\"URLs\",\n            info=\"Enter one or more URLs, by clicking the '+' button.\",\n            is_list=True,\n            tool_mode=True,\n        ),\n        DropdownInput(\n            name=\"format\",\n            display_name=\"Output format\",\n            info=\"Output format. Use 'Text' to extract the text from the HTML or 'Raw HTML' for the raw HTML content.\",\n            options=[\"Text\", \"Raw HTML\"],\n            value=\"Text\",\n        ),\n    ]\n\n    outputs = [\n        Output(display_name=\"Data\", name=\"data\", method=\"fetch_content\"),\n        Output(display_name=\"Text\", name=\"text\", method=\"fetch_content_text\"),\n    ]\n\n    def ensure_url(self, string: str) -> str:\n        \"\"\"Ensures the given string is a URL by adding 'http://' if it doesn't start with 'http://' or 'https://'.\n\n        Raises an error if the string is not a valid URL.\n\n        Parameters:\n            string (str): The string to be checked and possibly modified.\n\n        Returns:\n            str: The modified string that is ensured to be a URL.\n\n        Raises:\n            ValueError: If the string is not a valid URL.\n        \"\"\"\n        if not string.startswith((\"http://\", \"https://\")):\n            string = \"http://\" + string\n\n        # Basic URL validation regex\n        url_regex = re.compile(\n            r\"^(https?:\\/\\/)?\"  # optional protocol\n            r\"(www\\.)?\"  # optional www\n            r\"([a-zA-Z0-9.-]+)\"  # domain\n            r\"(\\.[a-zA-Z]{2,})?\"  # top-level domain\n            r\"(:\\d+)?\"  # optional port\n            r\"(\\/[^\\s]*)?$\",  # optional path\n            re.IGNORECASE,\n        )\n\n        if not url_regex.match(string):\n            msg = f\"Invalid URL: {string}\"\n            raise ValueError(msg)\n\n        return string\n\n    def fetch_content(self) -> list[Data]:\n        urls = [self.ensure_url(url.strip()) for url in self.urls if url.strip()]\n        if self.format == \"Raw HTML\":\n            loader = AsyncHtmlLoader(web_path=urls, encoding=\"utf-8\")\n        else:\n            loader = WebBaseLoader(web_paths=urls, encoding=\"utf-8\")\n        docs = loader.load()\n        data = [Data(text=doc.page_content, **doc.metadata) for doc in docs]\n        self.status = data\n        return data\n\n    def fetch_content_text(self) -> Message:\n        data = self.fetch_content()\n\n        result_string = data_to_text(\"{text}\", data)\n        self.status = result_string\n        return Message(text=result_string)\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false},"format":{"tool_mode":false,"trace_as_metadata":true,"options":["Text","Raw HTML"],"combobox":false,"required":false,"placeholder":"","show":true,"name":"format","value":"Text","display_name":"Output format","advanced":false,"dynamic":false,"info":"Output format. Use 'Text' to extract the text from the HTML or 'Raw HTML' for the raw HTML content.","title_case":false,"type":"str","_input_type":"DropdownInput"},"urls":{"tool_mode":true,"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":true,"required":false,"placeholder":"","show":true,"name":"urls","value":["https://developers.google.com/blockly/guides/configure/web/toolbox","https://developers.google.com/blockly/guides/create-custom-blocks/variables","https://github.com/katirasole/JSONLogic-Editor/blob/045de8566cec3a7d59a67bee15216ea02a7a8f48/menuly_2blocks.js"],"display_name":"URLs","advanced":false,"input_types":["Message"],"dynamic":false,"info":"Enter one or more URLs, by clicking the '+' button.","title_case":false,"type":"str","_input_type":"MessageTextInput"}},"description":"Fetch content from one or more URLs.","icon":"layout-template","base_classes":["Data","Message"],"display_name":"URL","documentation":"","custom_fields":{},"output_types":[],"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Data"],"selected":"Data","name":"data","display_name":"Data","method":"fetch_content","value":"__UNDEFINED__","cache":true},{"types":["Message"],"selected":"Message","name":"text","display_name":"Text","method":"fetch_content_text","value":"__UNDEFINED__","cache":true}],"field_order":["urls","format"],"beta":false,"legacy":false,"edited":false,"metadata":{},"tool_mode":false,"lf_version":"1.1.0"},"id":"URL-lCw7J","description":"Fetch content from one or more URLs.","display_name":"URL"},"selected":false,"width":320,"height":467,"positionAbsolute":{"x":-210.19347116991523,"y":-79.3465022859368},"dragging":false},{"id":"OpenAIModel-nQX38","type":"genericNode","position":{"x":829.6234348731521,"y":38},"data":{"type":"OpenAIModel","node":{"template":{"_type":"Component","output_parser":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"output_parser","value":"","display_name":"Output Parser","advanced":true,"input_types":["OutputParser"],"dynamic":false,"info":"The parser to use to parse the output of the model","title_case":false,"type":"other","_input_type":"HandleInput"},"api_key":{"load_from_db":true,"required":false,"placeholder":"","show":true,"name":"api_key","value":"OPENAI_API_KEY","display_name":"OpenAI API Key","advanced":false,"input_types":["Message"],"dynamic":false,"info":"The OpenAI API Key to use for the OpenAI model.","title_case":false,"password":true,"type":"str","_input_type":"SecretStrInput"},"code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"import operator\nfrom functools import reduce\n\nfrom langchain_openai import ChatOpenAI\nfrom pydantic.v1 import SecretStr\n\nfrom langflow.base.models.model import LCModelComponent\nfrom langflow.base.models.openai_constants import OPENAI_MODEL_NAMES\nfrom langflow.field_typing import LanguageModel\nfrom langflow.field_typing.range_spec import RangeSpec\nfrom langflow.inputs import BoolInput, DictInput, DropdownInput, FloatInput, IntInput, SecretStrInput, StrInput\nfrom langflow.inputs.inputs import HandleInput\n\n\nclass OpenAIModelComponent(LCModelComponent):\n    display_name = \"OpenAI\"\n    description = \"Generates text using OpenAI LLMs.\"\n    icon = \"OpenAI\"\n    name = \"OpenAIModel\"\n\n    inputs = [\n        *LCModelComponent._base_inputs,\n        IntInput(\n            name=\"max_tokens\",\n            display_name=\"Max Tokens\",\n            advanced=True,\n            info=\"The maximum number of tokens to generate. Set to 0 for unlimited tokens.\",\n            range_spec=RangeSpec(min=0, max=128000),\n        ),\n        DictInput(\n            name=\"model_kwargs\",\n            display_name=\"Model Kwargs\",\n            advanced=True,\n            info=\"Additional keyword arguments to pass to the model.\",\n        ),\n        BoolInput(\n            name=\"json_mode\",\n            display_name=\"JSON Mode\",\n            advanced=True,\n            info=\"If True, it will output JSON regardless of passing a schema.\",\n        ),\n        DictInput(\n            name=\"output_schema\",\n            is_list=True,\n            display_name=\"Schema\",\n            advanced=True,\n            info=\"The schema for the Output of the model. \"\n            \"You must pass the word JSON in the prompt. \"\n            \"If left blank, JSON mode will be disabled. [DEPRECATED]\",\n        ),\n        DropdownInput(\n            name=\"model_name\",\n            display_name=\"Model Name\",\n            advanced=False,\n            options=OPENAI_MODEL_NAMES,\n            value=OPENAI_MODEL_NAMES[0],\n        ),\n        StrInput(\n            name=\"openai_api_base\",\n            display_name=\"OpenAI API Base\",\n            advanced=True,\n            info=\"The base URL of the OpenAI API. \"\n            \"Defaults to https://api.openai.com/v1. \"\n            \"You can change this to use other APIs like JinaChat, LocalAI and Prem.\",\n        ),\n        SecretStrInput(\n            name=\"api_key\",\n            display_name=\"OpenAI API Key\",\n            info=\"The OpenAI API Key to use for the OpenAI model.\",\n            advanced=False,\n            value=\"OPENAI_API_KEY\",\n        ),\n        FloatInput(name=\"temperature\", display_name=\"Temperature\", value=0.1),\n        IntInput(\n            name=\"seed\",\n            display_name=\"Seed\",\n            info=\"The seed controls the reproducibility of the job.\",\n            advanced=True,\n            value=1,\n        ),\n        HandleInput(\n            name=\"output_parser\",\n            display_name=\"Output Parser\",\n            info=\"The parser to use to parse the output of the model\",\n            advanced=True,\n            input_types=[\"OutputParser\"],\n        ),\n    ]\n\n    def build_model(self) -> LanguageModel:  # type: ignore[type-var]\n        # self.output_schema is a list of dictionaries\n        # let's convert it to a dictionary\n        output_schema_dict: dict[str, str] = reduce(operator.ior, self.output_schema or {}, {})\n        openai_api_key = self.api_key\n        temperature = self.temperature\n        model_name: str = self.model_name\n        max_tokens = self.max_tokens\n        model_kwargs = self.model_kwargs or {}\n        openai_api_base = self.openai_api_base or \"https://api.openai.com/v1\"\n        json_mode = bool(output_schema_dict) or self.json_mode\n        seed = self.seed\n\n        api_key = SecretStr(openai_api_key).get_secret_value() if openai_api_key else None\n        output = ChatOpenAI(\n            max_tokens=max_tokens or None,\n            model_kwargs=model_kwargs,\n            model=model_name,\n            base_url=openai_api_base,\n            api_key=api_key,\n            temperature=temperature if temperature is not None else 0.1,\n            seed=seed,\n        )\n        if json_mode:\n            if output_schema_dict:\n                output = output.with_structured_output(schema=output_schema_dict, method=\"json_mode\")\n            else:\n                output = output.bind(response_format={\"type\": \"json_object\"})\n\n        return output\n\n    def _get_exception_message(self, e: Exception):\n        \"\"\"Get a message from an OpenAI exception.\n\n        Args:\n            e (Exception): The exception to get the message from.\n\n        Returns:\n            str: The message from the exception.\n        \"\"\"\n        try:\n            from openai import BadRequestError\n        except ImportError:\n            return None\n        if isinstance(e, BadRequestError):\n            message = e.body.get(\"message\")\n            if message:\n                return message\n        return None\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false},"input_value":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"input_value","value":"","display_name":"Input","advanced":false,"input_types":["Message"],"dynamic":false,"info":"","title_case":false,"type":"str","_input_type":"MessageInput"},"json_mode":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"json_mode","value":false,"display_name":"JSON Mode","advanced":true,"dynamic":false,"info":"If True, it will output JSON regardless of passing a schema.","title_case":false,"type":"bool","_input_type":"BoolInput"},"max_tokens":{"trace_as_metadata":true,"range_spec":{"step_type":"float","min":0,"max":128000,"step":0.1},"list":false,"required":false,"placeholder":"","show":true,"name":"max_tokens","value":"","display_name":"Max Tokens","advanced":true,"dynamic":false,"info":"The maximum number of tokens to generate. Set to 0 for unlimited tokens.","title_case":false,"type":"int","_input_type":"IntInput"},"model_kwargs":{"trace_as_input":true,"list":false,"required":false,"placeholder":"","show":true,"name":"model_kwargs","value":{},"display_name":"Model Kwargs","advanced":true,"dynamic":false,"info":"Additional keyword arguments to pass to the model.","title_case":false,"type":"dict","_input_type":"DictInput"},"model_name":{"tool_mode":false,"trace_as_metadata":true,"options":["gpt-4o-mini","gpt-4o","gpt-4-turbo","gpt-4-turbo-preview","gpt-4","gpt-3.5-turbo","gpt-3.5-turbo-0125"],"combobox":false,"required":false,"placeholder":"","show":true,"name":"model_name","value":"gpt-4o","display_name":"Model Name","advanced":false,"dynamic":false,"info":"","title_case":false,"type":"str","_input_type":"DropdownInput","load_from_db":false},"openai_api_base":{"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"openai_api_base","value":"","display_name":"OpenAI API Base","advanced":true,"dynamic":false,"info":"The base URL of the OpenAI API. Defaults to https://api.openai.com/v1. You can change this to use other APIs like JinaChat, LocalAI and Prem.","title_case":false,"type":"str","_input_type":"StrInput"},"output_schema":{"trace_as_input":true,"list":true,"required":false,"placeholder":"","show":true,"name":"output_schema","value":{},"display_name":"Schema","advanced":true,"dynamic":false,"info":"The schema for the Output of the model. You must pass the word JSON in the prompt. If left blank, JSON mode will be disabled. [DEPRECATED]","title_case":false,"type":"dict","_input_type":"DictInput"},"seed":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"seed","value":1,"display_name":"Seed","advanced":true,"dynamic":false,"info":"The seed controls the reproducibility of the job.","title_case":false,"type":"int","_input_type":"IntInput"},"stream":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"stream","value":false,"display_name":"Stream","advanced":false,"dynamic":false,"info":"Stream the response from the model. Streaming works only in Chat.","title_case":false,"type":"bool","_input_type":"BoolInput"},"system_message":{"tool_mode":false,"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"system_message","value":"","display_name":"System Message","advanced":false,"input_types":["Message"],"dynamic":false,"info":"System message to pass to the model.","title_case":false,"type":"str","_input_type":"MessageTextInput"},"temperature":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"temperature","value":0,"display_name":"Temperature","advanced":false,"dynamic":false,"info":"","title_case":false,"type":"float","_input_type":"FloatInput","load_from_db":false}},"description":"Generates text using OpenAI LLMs.","icon":"OpenAI","base_classes":["LanguageModel","Message"],"display_name":"OpenAI","documentation":"","custom_fields":{},"output_types":[],"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Message"],"selected":"Message","name":"text_output","display_name":"Text","method":"text_response","value":"__UNDEFINED__","cache":true,"required_inputs":[]},{"types":["LanguageModel"],"selected":"LanguageModel","name":"model_output","display_name":"Language Model","method":"build_model","value":"__UNDEFINED__","cache":true,"required_inputs":[]}],"field_order":["input_value","system_message","stream","max_tokens","model_kwargs","json_mode","output_schema","model_name","openai_api_base","api_key","temperature","seed","output_parser"],"beta":false,"legacy":false,"edited":false,"metadata":{},"tool_mode":false,"lf_version":"1.1.0"},"id":"OpenAIModel-nQX38","description":"Generates text using OpenAI LLMs.","display_name":"OpenAI"},"selected":false,"width":320,"height":671,"dragging":false},{"id":"Prompt-A7lUJ","type":"genericNode","position":{"x":366.38479352162614,"y":121.04278019908574},"data":{"type":"Prompt","node":{"template":{"_type":"Component","code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"from langflow.base.prompts.api_utils import process_prompt_template\nfrom langflow.custom import Component\nfrom langflow.inputs.inputs import DefaultPromptField\nfrom langflow.io import Output, PromptInput\nfrom langflow.schema.message import Message\nfrom langflow.template.utils import update_template_values\n\n\nclass PromptComponent(Component):\n    display_name: str = \"Prompt\"\n    description: str = \"Create a prompt template with dynamic variables.\"\n    icon = \"prompts\"\n    trace_type = \"prompt\"\n    name = \"Prompt\"\n\n    inputs = [\n        PromptInput(name=\"template\", display_name=\"Template\"),\n    ]\n\n    outputs = [\n        Output(display_name=\"Prompt Message\", name=\"prompt\", method=\"build_prompt\"),\n    ]\n\n    async def build_prompt(self) -> Message:\n        prompt = Message.from_template(**self._attributes)\n        self.status = prompt.text\n        return prompt\n\n    def _update_template(self, frontend_node: dict):\n        prompt_template = frontend_node[\"template\"][\"template\"][\"value\"]\n        custom_fields = frontend_node[\"custom_fields\"]\n        frontend_node_template = frontend_node[\"template\"]\n        _ = process_prompt_template(\n            template=prompt_template,\n            name=\"template\",\n            custom_fields=custom_fields,\n            frontend_node_template=frontend_node_template,\n        )\n        return frontend_node\n\n    def post_code_processing(self, new_frontend_node: dict, current_frontend_node: dict):\n        \"\"\"This function is called after the code validation is done.\"\"\"\n        frontend_node = super().post_code_processing(new_frontend_node, current_frontend_node)\n        template = frontend_node[\"template\"][\"template\"][\"value\"]\n        # Kept it duplicated for backwards compatibility\n        _ = process_prompt_template(\n            template=template,\n            name=\"template\",\n            custom_fields=frontend_node[\"custom_fields\"],\n            frontend_node_template=frontend_node[\"template\"],\n        )\n        # Now that template is updated, we need to grab any values that were set in the current_frontend_node\n        # and update the frontend_node with those values\n        update_template_values(new_template=frontend_node, previous_template=current_frontend_node[\"template\"])\n        return frontend_node\n\n    def _get_fallback_input(self, **kwargs):\n        return DefaultPromptField(**kwargs)\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false},"template":{"tool_mode":false,"trace_as_input":true,"list":false,"required":false,"placeholder":"","show":true,"name":"template","value":"You are a principal software engineer and your job is to create a complete DSL specification for {library}. \nYou are given the following documentation on {library} and your task is to extract all supported operations and provide an example for each.\nYou are ONLY to document the JSON parts, not the XML since that is deprecated.\nIgnore information that does not pertain to {library}. \nDo NOT document any information that is not outlined in the following documentation:\n---\n{documentation}\n\n---","display_name":"Template","advanced":false,"dynamic":false,"info":"","title_case":false,"type":"prompt","_input_type":"PromptInput","load_from_db":false},"library":{"field_type":"str","required":false,"placeholder":"","list":false,"show":true,"multiline":true,"value":"JsonLogic","fileTypes":[],"file_path":"","name":"library","display_name":"library","advanced":false,"input_types":["Message","Text"],"dynamic":false,"info":"","load_from_db":false,"title_case":false,"type":"str"},"documentation":{"field_type":"str","required":false,"placeholder":"","list":false,"show":true,"multiline":true,"value":"","fileTypes":[],"file_path":"","name":"documentation","display_name":"documentation","advanced":false,"input_types":["Message","Text"],"dynamic":false,"info":"","load_from_db":false,"title_case":false,"type":"str"}},"description":"Create a prompt template with dynamic variables.","icon":"prompts","is_input":null,"is_output":null,"is_composition":null,"base_classes":["Message"],"name":"","display_name":"Prompt","documentation":"","custom_fields":{"template":["library","documentation"]},"output_types":[],"full_path":null,"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Message"],"selected":"Message","name":"prompt","hidden":null,"display_name":"Prompt Message","method":"build_prompt","value":"__UNDEFINED__","cache":true,"required_inputs":null}],"field_order":["template"],"beta":false,"legacy":false,"error":null,"edited":false,"metadata":{},"tool_mode":false,"lf_version":"1.1.0"},"id":"Prompt-A7lUJ","description":"Create a prompt template with dynamic variables.","display_name":"Prompt"},"selected":false,"width":320,"height":431,"dragging":false},{"id":"OpenAIModel-scdKD","type":"genericNode","position":{"x":1087.474762833499,"y":1242.5351662026526},"data":{"type":"OpenAIModel","node":{"template":{"_type":"Component","output_parser":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"output_parser","value":"","display_name":"Output Parser","advanced":true,"input_types":["OutputParser"],"dynamic":false,"info":"The parser to use to parse the output of the model","title_case":false,"type":"other","_input_type":"HandleInput"},"api_key":{"load_from_db":true,"required":false,"placeholder":"","show":true,"name":"api_key","value":"OPENAI_API_KEY","display_name":"OpenAI API Key","advanced":false,"input_types":["Message"],"dynamic":false,"info":"The OpenAI API Key to use for the OpenAI model.","title_case":false,"password":true,"type":"str","_input_type":"SecretStrInput"},"code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"import operator\nfrom functools import reduce\n\nfrom langchain_openai import ChatOpenAI\nfrom pydantic.v1 import SecretStr\n\nfrom langflow.base.models.model import LCModelComponent\nfrom langflow.base.models.openai_constants import OPENAI_MODEL_NAMES\nfrom langflow.field_typing import LanguageModel\nfrom langflow.field_typing.range_spec import RangeSpec\nfrom langflow.inputs import BoolInput, DictInput, DropdownInput, FloatInput, IntInput, SecretStrInput, StrInput\nfrom langflow.inputs.inputs import HandleInput\n\n\nclass OpenAIModelComponent(LCModelComponent):\n    display_name = \"OpenAI\"\n    description = \"Generates text using OpenAI LLMs.\"\n    icon = \"OpenAI\"\n    name = \"OpenAIModel\"\n\n    inputs = [\n        *LCModelComponent._base_inputs,\n        IntInput(\n            name=\"max_tokens\",\n            display_name=\"Max Tokens\",\n            advanced=True,\n            info=\"The maximum number of tokens to generate. Set to 0 for unlimited tokens.\",\n            range_spec=RangeSpec(min=0, max=128000),\n        ),\n        DictInput(\n            name=\"model_kwargs\",\n            display_name=\"Model Kwargs\",\n            advanced=True,\n            info=\"Additional keyword arguments to pass to the model.\",\n        ),\n        BoolInput(\n            name=\"json_mode\",\n            display_name=\"JSON Mode\",\n            advanced=True,\n            info=\"If True, it will output JSON regardless of passing a schema.\",\n        ),\n        DictInput(\n            name=\"output_schema\",\n            is_list=True,\n            display_name=\"Schema\",\n            advanced=True,\n            info=\"The schema for the Output of the model. \"\n            \"You must pass the word JSON in the prompt. \"\n            \"If left blank, JSON mode will be disabled. [DEPRECATED]\",\n        ),\n        DropdownInput(\n            name=\"model_name\",\n            display_name=\"Model Name\",\n            advanced=False,\n            options=OPENAI_MODEL_NAMES,\n            value=OPENAI_MODEL_NAMES[0],\n        ),\n        StrInput(\n            name=\"openai_api_base\",\n            display_name=\"OpenAI API Base\",\n            advanced=True,\n            info=\"The base URL of the OpenAI API. \"\n            \"Defaults to https://api.openai.com/v1. \"\n            \"You can change this to use other APIs like JinaChat, LocalAI and Prem.\",\n        ),\n        SecretStrInput(\n            name=\"api_key\",\n            display_name=\"OpenAI API Key\",\n            info=\"The OpenAI API Key to use for the OpenAI model.\",\n            advanced=False,\n            value=\"OPENAI_API_KEY\",\n        ),\n        FloatInput(name=\"temperature\", display_name=\"Temperature\", value=0.1),\n        IntInput(\n            name=\"seed\",\n            display_name=\"Seed\",\n            info=\"The seed controls the reproducibility of the job.\",\n            advanced=True,\n            value=1,\n        ),\n        HandleInput(\n            name=\"output_parser\",\n            display_name=\"Output Parser\",\n            info=\"The parser to use to parse the output of the model\",\n            advanced=True,\n            input_types=[\"OutputParser\"],\n        ),\n    ]\n\n    def build_model(self) -> LanguageModel:  # type: ignore[type-var]\n        # self.output_schema is a list of dictionaries\n        # let's convert it to a dictionary\n        output_schema_dict: dict[str, str] = reduce(operator.ior, self.output_schema or {}, {})\n        openai_api_key = self.api_key\n        temperature = self.temperature\n        model_name: str = self.model_name\n        max_tokens = self.max_tokens\n        model_kwargs = self.model_kwargs or {}\n        openai_api_base = self.openai_api_base or \"https://api.openai.com/v1\"\n        json_mode = bool(output_schema_dict) or self.json_mode\n        seed = self.seed\n\n        api_key = SecretStr(openai_api_key).get_secret_value() if openai_api_key else None\n        output = ChatOpenAI(\n            max_tokens=max_tokens or None,\n            model_kwargs=model_kwargs,\n            model=model_name,\n            base_url=openai_api_base,\n            api_key=api_key,\n            temperature=temperature if temperature is not None else 0.1,\n            seed=seed,\n        )\n        if json_mode:\n            if output_schema_dict:\n                output = output.with_structured_output(schema=output_schema_dict, method=\"json_mode\")\n            else:\n                output = output.bind(response_format={\"type\": \"json_object\"})\n\n        return output\n\n    def _get_exception_message(self, e: Exception):\n        \"\"\"Get a message from an OpenAI exception.\n\n        Args:\n            e (Exception): The exception to get the message from.\n\n        Returns:\n            str: The message from the exception.\n        \"\"\"\n        try:\n            from openai import BadRequestError\n        except ImportError:\n            return None\n        if isinstance(e, BadRequestError):\n            message = e.body.get(\"message\")\n            if message:\n                return message\n        return None\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false},"input_value":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"input_value","value":"","display_name":"Input","advanced":false,"input_types":["Message"],"dynamic":false,"info":"","title_case":false,"type":"str","_input_type":"MessageInput"},"json_mode":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"json_mode","value":false,"display_name":"JSON Mode","advanced":true,"dynamic":false,"info":"If True, it will output JSON regardless of passing a schema.","title_case":false,"type":"bool","_input_type":"BoolInput"},"max_tokens":{"trace_as_metadata":true,"range_spec":{"step_type":"float","min":0,"max":128000,"step":0.1},"list":false,"required":false,"placeholder":"","show":true,"name":"max_tokens","value":"","display_name":"Max Tokens","advanced":true,"dynamic":false,"info":"The maximum number of tokens to generate. Set to 0 for unlimited tokens.","title_case":false,"type":"int","_input_type":"IntInput"},"model_kwargs":{"trace_as_input":true,"list":false,"required":false,"placeholder":"","show":true,"name":"model_kwargs","value":{},"display_name":"Model Kwargs","advanced":true,"dynamic":false,"info":"Additional keyword arguments to pass to the model.","title_case":false,"type":"dict","_input_type":"DictInput"},"model_name":{"tool_mode":false,"trace_as_metadata":true,"options":["gpt-4o-mini","gpt-4o","gpt-4-turbo","gpt-4-turbo-preview","gpt-4","gpt-3.5-turbo","gpt-3.5-turbo-0125"],"combobox":false,"required":false,"placeholder":"","show":true,"name":"model_name","value":"gpt-4o","display_name":"Model Name","advanced":false,"dynamic":false,"info":"","title_case":false,"type":"str","_input_type":"DropdownInput","load_from_db":false},"openai_api_base":{"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"openai_api_base","value":"","display_name":"OpenAI API Base","advanced":true,"dynamic":false,"info":"The base URL of the OpenAI API. Defaults to https://api.openai.com/v1. You can change this to use other APIs like JinaChat, LocalAI and Prem.","title_case":false,"type":"str","_input_type":"StrInput"},"output_schema":{"trace_as_input":true,"list":true,"required":false,"placeholder":"","show":true,"name":"output_schema","value":{},"display_name":"Schema","advanced":true,"dynamic":false,"info":"The schema for the Output of the model. You must pass the word JSON in the prompt. If left blank, JSON mode will be disabled. [DEPRECATED]","title_case":false,"type":"dict","_input_type":"DictInput"},"seed":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"seed","value":1,"display_name":"Seed","advanced":true,"dynamic":false,"info":"The seed controls the reproducibility of the job.","title_case":false,"type":"int","_input_type":"IntInput"},"stream":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"stream","value":false,"display_name":"Stream","advanced":false,"dynamic":false,"info":"Stream the response from the model. Streaming works only in Chat.","title_case":false,"type":"bool","_input_type":"BoolInput"},"system_message":{"tool_mode":false,"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"system_message","value":"","display_name":"System Message","advanced":false,"input_types":["Message"],"dynamic":false,"info":"System message to pass to the model.","title_case":false,"type":"str","_input_type":"MessageTextInput"},"temperature":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"temperature","value":0,"display_name":"Temperature","advanced":false,"dynamic":false,"info":"","title_case":false,"type":"float","_input_type":"FloatInput","load_from_db":false}},"description":"Generates text using OpenAI LLMs.","icon":"OpenAI","base_classes":["LanguageModel","Message"],"display_name":"OpenAI","documentation":"","custom_fields":{},"output_types":[],"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Message"],"selected":"Message","name":"text_output","display_name":"Text","method":"text_response","value":"__UNDEFINED__","cache":true,"required_inputs":[]},{"types":["LanguageModel"],"selected":"LanguageModel","name":"model_output","display_name":"Language Model","method":"build_model","value":"__UNDEFINED__","cache":true,"required_inputs":[]}],"field_order":["input_value","system_message","stream","max_tokens","model_kwargs","json_mode","output_schema","model_name","openai_api_base","api_key","temperature","seed","output_parser"],"beta":false,"legacy":false,"edited":false,"metadata":{},"tool_mode":false,"lf_version":"1.1.0"},"id":"OpenAIModel-scdKD","description":"Generates text using OpenAI LLMs.","display_name":"OpenAI"},"selected":false,"width":320,"height":671,"positionAbsolute":{"x":1087.474762833499,"y":1242.5351662026526},"dragging":false},{"id":"Prompt-7DbwR","type":"genericNode","position":{"x":697.9785501406234,"y":807.0231130957052},"data":{"type":"Prompt","node":{"template":{"_type":"Component","code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"from langflow.base.prompts.api_utils import process_prompt_template\nfrom langflow.custom import Component\nfrom langflow.inputs.inputs import DefaultPromptField\nfrom langflow.io import Output, PromptInput\nfrom langflow.schema.message import Message\nfrom langflow.template.utils import update_template_values\n\n\nclass PromptComponent(Component):\n    display_name: str = \"Prompt\"\n    description: str = \"Create a prompt template with dynamic variables.\"\n    icon = \"prompts\"\n    trace_type = \"prompt\"\n    name = \"Prompt\"\n\n    inputs = [\n        PromptInput(name=\"template\", display_name=\"Template\"),\n    ]\n\n    outputs = [\n        Output(display_name=\"Prompt Message\", name=\"prompt\", method=\"build_prompt\"),\n    ]\n\n    async def build_prompt(self) -> Message:\n        prompt = Message.from_template(**self._attributes)\n        self.status = prompt.text\n        return prompt\n\n    def _update_template(self, frontend_node: dict):\n        prompt_template = frontend_node[\"template\"][\"template\"][\"value\"]\n        custom_fields = frontend_node[\"custom_fields\"]\n        frontend_node_template = frontend_node[\"template\"]\n        _ = process_prompt_template(\n            template=prompt_template,\n            name=\"template\",\n            custom_fields=custom_fields,\n            frontend_node_template=frontend_node_template,\n        )\n        return frontend_node\n\n    def post_code_processing(self, new_frontend_node: dict, current_frontend_node: dict):\n        \"\"\"This function is called after the code validation is done.\"\"\"\n        frontend_node = super().post_code_processing(new_frontend_node, current_frontend_node)\n        template = frontend_node[\"template\"][\"template\"][\"value\"]\n        # Kept it duplicated for backwards compatibility\n        _ = process_prompt_template(\n            template=template,\n            name=\"template\",\n            custom_fields=frontend_node[\"custom_fields\"],\n            frontend_node_template=frontend_node[\"template\"],\n        )\n        # Now that template is updated, we need to grab any values that were set in the current_frontend_node\n        # and update the frontend_node with those values\n        update_template_values(new_template=frontend_node, previous_template=current_frontend_node[\"template\"])\n        return frontend_node\n\n    def _get_fallback_input(self, **kwargs):\n        return DefaultPromptField(**kwargs)\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false},"template":{"tool_mode":false,"trace_as_input":true,"list":false,"required":false,"placeholder":"","show":true,"name":"template","value":"You are a principal software engineer reviewing the work of another engineer. You will provide an honest review of their work, outlining mistakes they made, if any. If they made no mistakes, respond \"LGTM\".\nYou will review their task description and their output and ensure they followed their task correctly.\n\nTHEIR TASK:\n---\n{task}\n---\n\nTHEIR OUTPUT:\n---\n{output}\n---\n","display_name":"Template","advanced":false,"dynamic":false,"info":"","title_case":false,"type":"prompt","_input_type":"PromptInput","load_from_db":false},"task":{"field_type":"str","required":false,"placeholder":"","list":false,"show":true,"multiline":true,"value":"","fileTypes":[],"file_path":"","name":"task","display_name":"task","advanced":false,"input_types":["Message","Text"],"dynamic":false,"info":"","load_from_db":false,"title_case":false,"type":"str"},"output":{"field_type":"str","required":false,"placeholder":"","list":false,"show":true,"multiline":true,"value":"","fileTypes":[],"file_path":"","name":"output","display_name":"output","advanced":false,"input_types":["Message","Text"],"dynamic":false,"info":"","load_from_db":false,"title_case":false,"type":"str"}},"description":"Create a prompt template with dynamic variables.","icon":"prompts","is_input":null,"is_output":null,"is_composition":null,"base_classes":["Message"],"name":"","display_name":"Reviewer Prompt ","documentation":"","custom_fields":{"template":["task","output"]},"output_types":[],"full_path":null,"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Message"],"selected":"Message","name":"prompt","hidden":null,"display_name":"Prompt Message","method":"build_prompt","value":"__UNDEFINED__","cache":true,"required_inputs":null}],"field_order":["template"],"beta":false,"legacy":false,"error":null,"edited":false,"metadata":{},"tool_mode":false,"lf_version":"1.1.0"},"id":"Prompt-7DbwR","description":"Create a prompt template with dynamic variables.","display_name":"Reviewer Prompt "},"selected":false,"width":320,"height":431,"positionAbsolute":{"x":697.9785501406234,"y":807.0231130957052},"dragging":false},{"id":"TextInput-X22Qw","type":"genericNode","position":{"x":-205.98800162289126,"y":442.65071236883},"data":{"node":{"template":{"_type":"Component","code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"from langflow.base.io.text import TextComponent\nfrom langflow.io import MultilineInput, Output\nfrom langflow.schema.message import Message\n\n\nclass TextInputComponent(TextComponent):\n    display_name = \"Text Input\"\n    description = \"Get text inputs from the Playground.\"\n    icon = \"type\"\n    name = \"TextInput\"\n\n    inputs = [\n        MultilineInput(\n            name=\"input_value\",\n            display_name=\"Text\",\n            info=\"Text to be passed as input.\",\n        ),\n    ]\n    outputs = [\n        Output(display_name=\"Text\", name=\"text\", method=\"text_response\"),\n    ]\n\n    def text_response(self) -> Message:\n        return Message(\n            text=self.input_value,\n        )\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false},"input_value":{"tool_mode":false,"trace_as_input":true,"multiline":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"input_value","value":"Blocky","display_name":"Text","advanced":false,"input_types":["Message"],"dynamic":false,"info":"Text to be passed as input.","title_case":false,"type":"str","_input_type":"MultilineInput"}},"description":"Get text inputs from the Playground.","icon":"type","base_classes":["Message"],"display_name":"Library Input","documentation":"","custom_fields":{},"output_types":[],"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Message"],"selected":"Message","name":"text","display_name":"Text","method":"text_response","value":"__UNDEFINED__","cache":true}],"field_order":["input_value"],"beta":false,"legacy":false,"edited":false,"metadata":{},"tool_mode":false,"lf_version":"1.1.0"},"type":"TextInput","id":"TextInput-X22Qw"},"selected":false,"width":320,"height":233,"positionAbsolute":{"x":-205.98800162289126,"y":442.65071236883},"dragging":false},{"id":"URL-O3B96","type":"genericNode","position":{"x":1919.8951406799342,"y":-404.6977417650489},"data":{"type":"URL","node":{"template":{"_type":"Component","code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"import re\n\nfrom langchain_community.document_loaders import AsyncHtmlLoader, WebBaseLoader\n\nfrom langflow.custom import Component\nfrom langflow.helpers.data import data_to_text\nfrom langflow.io import DropdownInput, MessageTextInput, Output\nfrom langflow.schema import Data\nfrom langflow.schema.message import Message\n\n\nclass URLComponent(Component):\n    display_name = \"URL\"\n    description = \"Fetch content from one or more URLs.\"\n    icon = \"layout-template\"\n    name = \"URL\"\n\n    inputs = [\n        MessageTextInput(\n            name=\"urls\",\n            display_name=\"URLs\",\n            info=\"Enter one or more URLs, by clicking the '+' button.\",\n            is_list=True,\n            tool_mode=True,\n        ),\n        DropdownInput(\n            name=\"format\",\n            display_name=\"Output format\",\n            info=\"Output format. Use 'Text' to extract the text from the HTML or 'Raw HTML' for the raw HTML content.\",\n            options=[\"Text\", \"Raw HTML\"],\n            value=\"Text\",\n        ),\n    ]\n\n    outputs = [\n        Output(display_name=\"Data\", name=\"data\", method=\"fetch_content\"),\n        Output(display_name=\"Text\", name=\"text\", method=\"fetch_content_text\"),\n    ]\n\n    def ensure_url(self, string: str) -> str:\n        \"\"\"Ensures the given string is a URL by adding 'http://' if it doesn't start with 'http://' or 'https://'.\n\n        Raises an error if the string is not a valid URL.\n\n        Parameters:\n            string (str): The string to be checked and possibly modified.\n\n        Returns:\n            str: The modified string that is ensured to be a URL.\n\n        Raises:\n            ValueError: If the string is not a valid URL.\n        \"\"\"\n        if not string.startswith((\"http://\", \"https://\")):\n            string = \"http://\" + string\n\n        # Basic URL validation regex\n        url_regex = re.compile(\n            r\"^(https?:\\/\\/)?\"  # optional protocol\n            r\"(www\\.)?\"  # optional www\n            r\"([a-zA-Z0-9.-]+)\"  # domain\n            r\"(\\.[a-zA-Z]{2,})?\"  # top-level domain\n            r\"(:\\d+)?\"  # optional port\n            r\"(\\/[^\\s]*)?$\",  # optional path\n            re.IGNORECASE,\n        )\n\n        if not url_regex.match(string):\n            msg = f\"Invalid URL: {string}\"\n            raise ValueError(msg)\n\n        return string\n\n    def fetch_content(self) -> list[Data]:\n        urls = [self.ensure_url(url.strip()) for url in self.urls if url.strip()]\n        if self.format == \"Raw HTML\":\n            loader = AsyncHtmlLoader(web_path=urls, encoding=\"utf-8\")\n        else:\n            loader = WebBaseLoader(web_paths=urls, encoding=\"utf-8\")\n        docs = loader.load()\n        data = [Data(text=doc.page_content, **doc.metadata) for doc in docs]\n        self.status = data\n        return data\n\n    def fetch_content_text(self) -> Message:\n        data = self.fetch_content()\n\n        result_string = data_to_text(\"{text}\", data)\n        self.status = result_string\n        return Message(text=result_string)\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false},"format":{"tool_mode":false,"trace_as_metadata":true,"options":["Text","Raw HTML"],"combobox":false,"required":false,"placeholder":"","show":true,"name":"format","value":"Text","display_name":"Output format","advanced":false,"dynamic":false,"info":"Output format. Use 'Text' to extract the text from the HTML or 'Raw HTML' for the raw HTML content.","title_case":false,"type":"str","_input_type":"DropdownInput"},"urls":{"tool_mode":true,"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":true,"required":false,"placeholder":"","show":true,"name":"urls","value":["https://github.com/katirasole/JSONLogic-Editor/blob/045de8566cec3a7d59a67bee15216ea02a7a8f48/menuly_2blocks.js"],"display_name":"URLs","advanced":false,"input_types":["Message"],"dynamic":false,"info":"Enter one or more URLs, by clicking the '+' button.","title_case":false,"type":"str","_input_type":"MessageTextInput"}},"description":"Fetch content from one or more URLs.","icon":"layout-template","base_classes":["Data","Message"],"display_name":"URL","documentation":"","custom_fields":{},"output_types":[],"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Data"],"selected":"Data","name":"data","display_name":"Data","method":"fetch_content","value":"__UNDEFINED__","cache":true},{"types":["Message"],"selected":"Message","name":"text","display_name":"Text","method":"fetch_content_text","value":"__UNDEFINED__","cache":true}],"field_order":["urls","format"],"beta":false,"legacy":false,"edited":false,"metadata":{},"tool_mode":false,"lf_version":"1.1.0"},"id":"URL-O3B96","description":"Fetch content from one or more URLs.","display_name":"URL"},"selected":false,"width":320,"height":367,"positionAbsolute":{"x":1919.8951406799342,"y":-404.6977417650489},"dragging":false},{"id":"Prompt-mNvAI","type":"genericNode","position":{"x":1811.7126542316948,"y":459.5898549298238},"data":{"type":"Prompt","node":{"template":{"_type":"Component","code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"from langflow.base.prompts.api_utils import process_prompt_template\nfrom langflow.custom import Component\nfrom langflow.inputs.inputs import DefaultPromptField\nfrom langflow.io import Output, PromptInput\nfrom langflow.schema.message import Message\nfrom langflow.template.utils import update_template_values\n\n\nclass PromptComponent(Component):\n    display_name: str = \"Prompt\"\n    description: str = \"Create a prompt template with dynamic variables.\"\n    icon = \"prompts\"\n    trace_type = \"prompt\"\n    name = \"Prompt\"\n\n    inputs = [\n        PromptInput(name=\"template\", display_name=\"Template\"),\n    ]\n\n    outputs = [\n        Output(display_name=\"Prompt Message\", name=\"prompt\", method=\"build_prompt\"),\n    ]\n\n    async def build_prompt(self) -> Message:\n        prompt = Message.from_template(**self._attributes)\n        self.status = prompt.text\n        return prompt\n\n    def _update_template(self, frontend_node: dict):\n        prompt_template = frontend_node[\"template\"][\"template\"][\"value\"]\n        custom_fields = frontend_node[\"custom_fields\"]\n        frontend_node_template = frontend_node[\"template\"]\n        _ = process_prompt_template(\n            template=prompt_template,\n            name=\"template\",\n            custom_fields=custom_fields,\n            frontend_node_template=frontend_node_template,\n        )\n        return frontend_node\n\n    def post_code_processing(self, new_frontend_node: dict, current_frontend_node: dict):\n        \"\"\"This function is called after the code validation is done.\"\"\"\n        frontend_node = super().post_code_processing(new_frontend_node, current_frontend_node)\n        template = frontend_node[\"template\"][\"template\"][\"value\"]\n        # Kept it duplicated for backwards compatibility\n        _ = process_prompt_template(\n            template=template,\n            name=\"template\",\n            custom_fields=frontend_node[\"custom_fields\"],\n            frontend_node_template=frontend_node[\"template\"],\n        )\n        # Now that template is updated, we need to grab any values that were set in the current_frontend_node\n        # and update the frontend_node with those values\n        update_template_values(new_template=frontend_node, previous_template=current_frontend_node[\"template\"])\n        return frontend_node\n\n    def _get_fallback_input(self, **kwargs):\n        return DefaultPromptField(**kwargs)\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false},"template":{"tool_mode":false,"trace_as_input":true,"list":false,"required":false,"placeholder":"","show":true,"name":"template","value":"You are a principal software engineer and your job is to create a pure converter function in Typescript that maps from JsonLogic to Blockly blocks.\n\nWe have some old code that does this, documentation on JsonLogic, and documentation on Blockly.\nI recommend FIRST extracting the data from JsonLogic into pure Json that describes the necessary Blockly block types and connections\nThen writing a function that can process that Json data and build the Blockly blocks. Declare your typescript types, and keep the code as clean as possible.\n\nBlockly Documentation:\n---\n{blockly_documentation}\n\n---\n\nJsonLogic Documentation:\n---\n{json_logic_documentation}\n\n---\n\nOld code:\n{old_code}","display_name":"Template","advanced":false,"dynamic":false,"info":"","title_case":false,"type":"prompt","_input_type":"PromptInput","load_from_db":false},"blockly_documentation":{"field_type":"str","required":false,"placeholder":"","list":false,"show":true,"multiline":true,"value":"","fileTypes":[],"file_path":"","name":"blockly_documentation","display_name":"blockly_documentation","advanced":false,"input_types":["Message","Text"],"dynamic":false,"info":"","load_from_db":false,"title_case":false,"type":"str"},"json_logic_documentation":{"field_type":"str","required":false,"placeholder":"","list":false,"show":true,"multiline":true,"value":"","fileTypes":[],"file_path":"","name":"json_logic_documentation","display_name":"json_logic_documentation","advanced":false,"input_types":["Message","Text"],"dynamic":false,"info":"","load_from_db":false,"title_case":false,"type":"str"},"old_code":{"field_type":"str","required":false,"placeholder":"","list":false,"show":true,"multiline":true,"value":"","fileTypes":[],"file_path":"","name":"old_code","display_name":"old_code","advanced":false,"input_types":["Message","Text"],"dynamic":false,"info":"","load_from_db":false,"title_case":false,"type":"str"}},"description":"Create a prompt template with dynamic variables.","icon":"prompts","is_input":null,"is_output":null,"is_composition":null,"base_classes":["Message"],"name":"","display_name":"Prompt","documentation":"","custom_fields":{"template":["blockly_documentation","json_logic_documentation","old_code"]},"output_types":[],"full_path":null,"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Message"],"selected":"Message","name":"prompt","hidden":null,"display_name":"Prompt Message","method":"build_prompt","value":"__UNDEFINED__","cache":true,"required_inputs":null}],"field_order":["template"],"beta":false,"legacy":false,"error":null,"edited":false,"metadata":{},"tool_mode":false,"lf_version":"1.1.0"},"id":"Prompt-mNvAI","description":"Create a prompt template with dynamic variables.","display_name":"Prompt"},"selected":false,"width":320,"height":517,"positionAbsolute":{"x":1811.7126542316948,"y":459.5898549298238},"dragging":false},{"id":"TextInput-AQXSV","type":"genericNode","position":{"x":1388.4558680668654,"y":555.5664504320528},"data":{"node":{"template":{"_type":"Component","code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"from langflow.base.io.text import TextComponent\nfrom langflow.io import MultilineInput, Output\nfrom langflow.schema.message import Message\n\n\nclass TextInputComponent(TextComponent):\n    display_name = \"Text Input\"\n    description = \"Get text inputs from the Playground.\"\n    icon = \"type\"\n    name = \"TextInput\"\n\n    inputs = [\n        MultilineInput(\n            name=\"input_value\",\n            display_name=\"Text\",\n            info=\"Text to be passed as input.\",\n        ),\n    ]\n    outputs = [\n        Output(display_name=\"Text\", name=\"text\", method=\"text_response\"),\n    ]\n\n    def text_response(self) -> Message:\n        return Message(\n            text=self.input_value,\n        )\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false},"input_value":{"tool_mode":false,"trace_as_input":true,"multiline":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"input_value","value":"Here is a complete DSL specification for JsonLogic, including all supported operations and examples for each:  ### Accessing Data  - **var**: Retrieve data from the provided data object.    ```json   { \"var\": [\"a\"] }   ```    Data: `{ \"a\": 1, \"b\": 2 }`   Result: `1`  - **missing**: Returns an array of any keys that are missing from the data object.    ```json   { \"missing\": [\"a\", \"b\"] }   ```    Data: `{ \"a\": \"apple\", \"c\": \"carrot\" }`   Result: `[\"b\"]`  - **missing_some**: Returns an empty array if the minimum number of keys is met, or an array of the missing keys otherwise.   ```json   { \"missing_some\": [1, [\"a\", \"b\", \"c\"]] }   ```   Data: `{ \"a\": \"apple\" }`   Result: `[]`  ### Logic and Boolean Operations  - **if**: Conditional logic.    ```json   { \"if\": [true, \"yes\", \"no\"] }   ```    Result: `\"yes\"`  - **==**: Tests equality, with type coercion.    ```json   { \"==\": [1, \"1\"] }   ```    Result: `true`  - **===**: Tests strict equality.    ```json   { \"===\": [1, \"1\"] }   ```    Result: `false`  - **!=**: Tests not-equal, with type coercion.    ```json   { \"!=\": [1, 2] }   ```    Result: `true`  - **!==**: Tests strict not-equal.    ```json   { \"!==\": [1, \"1\"] }   ```    Result: `true`  - **!**: Logical negation.    ```json   { \"!\": true }   ```    Result: `false`  - **!!**: Double negation, or “cast to a boolean.”    ```json   { \"!!\": [\"0\"] }   ```    Result: `true`  - **or**: Returns the first truthy argument, or the last argument.    ```json   { \"or\": [false, \"a\"] }   ```    Result: `\"a\"`  - **and**: Returns the first falsy argument, or the last argument.   ```json   { \"and\": [true, \"a\", 3] }   ```   Result: `3`  ### Numeric Operations  - **>**: Greater than.    ```json   { \">\": [2, 1] }   ```    Result: `true`  - **>=**: Greater than or equal to.    ```json   { \">=\": [1, 1] }   ```    Result: `true`  - **<**: Less than.    ```json   { \"<\": [1, 2] }   ```    Result: `true`  - **<=**: Less than or equal to.    ```json   { \"<=\": [1, 1] }   ```    Result: `true`  - **Between**: Test that one value is between two others.    ```json   { \"<\": [1, 2, 3] }   ```    Result: `true`  - **max**: Return the maximum from a list of values.    ```json   { \"max\": [1, 2, 3] }   ```    Result: `3`  - **min**: Return the minimum from a list of values.    ```json   { \"min\": [1, 2, 3] }   ```    Result: `1`  - **+**: Addition.    ```json   { \"+\": [4, 2] }   ```    Result: `6`  - **-**: Subtraction.    ```json   { \"-\": [4, 2] }   ```    Result: `2`  - **\\***: Multiplication.    ```json   { \"*\": [4, 2] }   ```    Result: `8`  - **/**: Division.    ```json   { \"/\": [4, 2] }   ```    Result: `2`  - **%**: Modulo.   ```json   { \"%\": [101, 2] }   ```   Result: `1`  ### Array Operations  - **map**: Perform an action on every member of an array.    ```json   { \"map\": [{ \"var\": \"integers\" }, { \"*\": [{ \"var\": \"\" }, 2] }] }   ```    Data: `{ \"integers\": [1, 2, 3, 4, 5] }`   Result: `[2, 4, 6, 8, 10]`  - **filter**: Keep only elements of the array that pass a test.    ```json   { \"filter\": [{ \"var\": \"integers\" }, { \"%\": [{ \"var\": \"\" }, 2] }] }   ```    Data: `{ \"integers\": [1, 2, 3, 4, 5] }`   Result: `[1, 3, 5]`  - **reduce**: Combine all the elements in an array into a single value.    ```json   { \"reduce\": [{ \"var\": \"integers\" }, { \"+\": [{ \"var\": \"current\" }, { \"var\": \"accumulator\" }] }, 0] }   ```    Data: `{ \"integers\": [1, 2, 3, 4, 5] }`   Result: `15`  - **all**: Test if all elements in an array pass a test.    ```json   { \"all\": [[1, 2, 3], { \">\": [{ \"var\": \"\" }, 0] }] }   ```    Result: `true`  - **none**: Test if no elements in an array pass a test.    ```json   { \"none\": [[-3, -2, -1], { \">\": [{ \"var\": \"\" }, 0] }] }   ```    Result: `true`  - **some**: Test if some elements in an array pass a test.    ```json   { \"some\": [[-1, 0, 1], { \">\": [{ \"var\": \"\" }, 0] }] }   ```    Result: `true`  - **merge**: Merge one or more arrays into one array.    ```json   {     \"merge\": [       [1, 2],       [3, 4]     ]   }   ```    Result: `[1, 2, 3, 4]`  - **in**: Test if the first argument is a member of the array.   ```json   { \"in\": [\"Ringo\", [\"John\", \"Paul\", \"George\", \"Ringo\"]] }   ```   Result: `true`  ### String Operations  - **in**: Test if the first argument is a substring of the second.    ```json   { \"in\": [\"Spring\", \"Springfield\"] }   ```    Result: `true`  - **cat**: Concatenate all the supplied arguments.    ```json   { \"cat\": [\"I love\", \" pie\"] }   ```    Result: `\"I love pie\"`  - **substr**: Get a portion of a string.   ```json   { \"substr\": [\"jsonlogic\", 4] }   ```   Result: `\"logic\"`  ### Miscellaneous  - **log**: Logs the first value to console, then passes it through unmodified.   ```json   { \"log\": \"apple\" }   ```   Result: `\"apple\"` (Check your developer console!)  This specification provides a comprehensive overview of the operations supported by JsonLogic, along with examples to illustrate their usage.","display_name":"Text","advanced":false,"input_types":["Message"],"dynamic":false,"info":"Text to be passed as input.","title_case":false,"type":"str","_input_type":"MultilineInput"}},"description":"Get text inputs from the Playground.","icon":"type","base_classes":["Message"],"display_name":"JsonLogic","documentation":"","custom_fields":{},"output_types":[],"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Message"],"selected":"Message","name":"text","display_name":"Text","method":"text_response","value":"__UNDEFINED__","cache":true}],"field_order":["input_value"],"beta":false,"legacy":false,"edited":false,"metadata":{},"tool_mode":false,"lf_version":"1.1.0"},"type":"TextInput","id":"TextInput-AQXSV"},"selected":false,"width":320,"height":233,"positionAbsolute":{"x":1388.4558680668654,"y":555.5664504320528},"dragging":false},{"id":"TextInput-7Cjz9","type":"genericNode","position":{"x":1389.4156340218876,"y":862.6915560391852},"data":{"node":{"template":{"_type":"Component","code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"from langflow.base.io.text import TextComponent\nfrom langflow.io import MultilineInput, Output\nfrom langflow.schema.message import Message\n\n\nclass TextInputComponent(TextComponent):\n    display_name = \"Text Input\"\n    description = \"Get text inputs from the Playground.\"\n    icon = \"type\"\n    name = \"TextInput\"\n\n    inputs = [\n        MultilineInput(\n            name=\"input_value\",\n            display_name=\"Text\",\n            info=\"Text to be passed as input.\",\n        ),\n    ]\n    outputs = [\n        Output(display_name=\"Text\", name=\"text\", method=\"text_response\"),\n    ]\n\n    def text_response(self) -> Message:\n        return Message(\n            text=self.input_value,\n        )\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false},"input_value":{"tool_mode":false,"trace_as_input":true,"multiline":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"input_value","value":"Based on the provided documentation, here is a summary of the supported operations in Blocky using JSON, along with examples for each:  1. **Toolbox Definition**:     - **Flyout Toolbox**: A simple toolbox without categories.       ```json      {        \"kind\": \"flyoutToolbox\",        \"contents\": [          {            \"kind\": \"block\",            \"type\": \"controls_if\"          },          {            \"kind\": \"block\",            \"type\": \"controls_whileUntil\"          }        ]      }      ```     - **Category Toolbox**: A toolbox with categories.      ```json      {        \"kind\": \"categoryToolbox\",        \"contents\": [          {            \"kind\": \"category\",            \"name\": \"Control\",            \"contents\": [              {                \"kind\": \"block\",                \"type\": \"controls_if\"              }            ]          },          {            \"kind\": \"category\",            \"name\": \"Logic\",            \"contents\": [              {                \"kind\": \"block\",                \"type\": \"logic_compare\"              },              {                \"kind\": \"block\",                \"type\": \"logic_operation\"              },              {                \"kind\": \"block\",                \"type\": \"logic_boolean\"              }            ]          }        ]      }      ```  2. **Nested Categories**:     - Categories can be nested within other categories.      ```json      {        \"kind\": \"categoryToolbox\",        \"contents\": [          {            \"kind\": \"category\",            \"name\": \"Core\",            \"contents\": [              {                \"kind\": \"block\",                \"type\": \"controls_if\"              },              {                \"kind\": \"block\",                \"type\": \"logic_compare\"              }            ]          },          {            \"kind\": \"category\",            \"name\": \"Custom\",            \"contents\": [              {                \"kind\": \"block\",                \"type\": \"start\"              },              {                \"kind\": \"category\",                \"name\": \"Move\",                \"contents\": [                  {                    \"kind\": \"block\",                    \"type\": \"move_forward\"                  }                ]              },              {                \"kind\": \"category\",                \"name\": \"Turn\",                \"contents\": [                  {                    \"kind\": \"block\",                    \"type\": \"turn_left\"                  }                ]              }            ]          }        ]      }      ```  3. **Dynamic Categories**:     - Categories that are dynamically populated based on a function.      ```json      {        \"kind\": \"category\",        \"name\": \"Colours\",        \"custom\": \"COLOUR_PALETTE\"      }      ```  4. **Built-in Dynamic Categories**:     - Predefined dynamic categories for variables and procedures.      ```json      {        \"kind\": \"category\",        \"name\": \"Variables\",        \"custom\": \"VARIABLE\"      },      {        \"kind\": \"category\",        \"name\": \"Variables\",        \"custom\": \"VARIABLE_DYNAMIC\"      },      {        \"kind\": \"category\",        \"name\": \"Functions\",        \"custom\": \"PROCEDURE\"      }      ```  5. **Preset Blocks**:     - Blocks with preset values or connected blocks.      ```json      {        \"kind\": \"flyoutToolbox\",        \"contents\": [          {            \"kind\": \"block\",            \"type\": \"logic_boolean\"          },          {            \"kind\": \"block\",            \"type\": \"math_number\",            \"fields\": {              \"NUM\": 42            }          },          {            \"kind\": \"block\",            \"type\": \"controls_for\",            \"inputs\": {              \"FROM\": {                \"block\": {                  \"type\": \"math_number\",                  \"fields\": {                    \"NUM\": 1                  }                }              },              \"TO\": {                \"block\": {                  \"type\": \"math_number\",                  \"fields\": {                    \"NUM\": 10                  }                }              },              \"BY\": {                \"block\": {                  \"type\": \"math_number\",                  \"fields\": {                    \"NUM\": 1                  }                }              }            }          },          {            \"kind\": \"block\",            \"type\": \"math_arithmetic\",            \"fields\": {              \"OP\": \"ADD\"            },            \"inputs\": {              \"A\": {                \"shadow\": {                  \"type\": \"math_number\",                  \"fields\": {                    \"NUM\": 1                  }                }              },              \"B\": {                \"shadow\": {                  \"type\": \"math_number\",                  \"fields\": {                    \"NUM\": 1                  }                }              }            }          }        ]      }      ```  6. **Disabled Blocks**:     - Blocks that cannot be dragged from the toolbox.      ```json      {        \"kind\": \"flyoutToolbox\",        \"contents\": [          {            \"kind\": \"block\",            \"type\": \"math_number\"          },          {            \"kind\": \"block\",            \"type\": \"math_arithmetic\"          },          {            \"kind\": \"block\",            \"type\": \"math_single\",            \"disabled\": \"true\"          }        ]      }      ```  7. **Separators**:     - Adding separators between blocks or categories.      ```json      {        \"kind\": \"flyoutToolbox\",        \"contents\": [          {            \"kind\": \"block\",            \"type\": \"math_number\"          },          {            \"kind\": \"sep\",            \"gap\": \"32\"          },          {            \"kind\": \"block\",            \"blockxml\": \"<block type='math_arithmetic'><field name='OP'>ADD</field></block>\"          },          {            \"kind\": \"sep\",            \"gap\": \"8\"          },          {            \"kind\": \"block\",            \"blockxml\": \"<block type='math_arithmetic'><field name='OP'>MINUS</field></block>\"          }        ]      }      ```  8. **Buttons and Labels**:    - Adding buttons and labels to the toolbox.      ```json      {        \"kind\": \"flyoutToolbox\",        \"contents\": [          {            \"kind\": \"block\",            \"type\": \"logic_operation\"          },          {            \"kind\": \"label\",            \"text\": \"A label\",            \"web-class\": \"myLabelStyle\"          },          {            \"kind\": \"label\",            \"text\": \"Another label\"          },          {            \"kind\": \"block\",            \"type\": \"logic_negate\"          },          {            \"kind\": \"button\",            \"text\": \"A button\",            \"callbackKey\": \"myFirstButtonPressed\"          },          {            \"kind\": \"block\",            \"type\": \"logic_boolean\"          }        ]      }      ```  These examples illustrate how to define various elements and configurations in a Blockly toolbox using JSON.","display_name":"Text","advanced":false,"input_types":["Message"],"dynamic":false,"info":"Text to be passed as input.","title_case":false,"type":"str","_input_type":"MultilineInput"}},"description":"Get text inputs from the Playground.","icon":"type","base_classes":["Message"],"display_name":"Blockly","documentation":"","custom_fields":{},"output_types":[],"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Message"],"selected":"Message","name":"text","display_name":"Text","method":"text_response","value":"__UNDEFINED__","cache":true}],"field_order":["input_value"],"beta":false,"legacy":false,"edited":false,"metadata":{},"tool_mode":false,"lf_version":"1.1.0"},"type":"TextInput","id":"TextInput-7Cjz9"},"selected":false,"width":320,"height":233,"dragging":false,"positionAbsolute":{"x":1389.4156340218876,"y":862.6915560391852}},{"id":"OpenAIModel-wT8PD","type":"genericNode","position":{"x":2346.2008802910887,"y":521.0123692316673},"data":{"type":"OpenAIModel","node":{"template":{"_type":"Component","output_parser":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"output_parser","value":"","display_name":"Output Parser","advanced":true,"input_types":["OutputParser"],"dynamic":false,"info":"The parser to use to parse the output of the model","title_case":false,"type":"other","_input_type":"HandleInput"},"api_key":{"load_from_db":true,"required":false,"placeholder":"","show":true,"name":"api_key","value":"OPENAI_API_KEY","display_name":"OpenAI API Key","advanced":false,"input_types":["Message"],"dynamic":false,"info":"The OpenAI API Key to use for the OpenAI model.","title_case":false,"password":true,"type":"str","_input_type":"SecretStrInput"},"code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"import operator\nfrom functools import reduce\n\nfrom langchain_openai import ChatOpenAI\nfrom pydantic.v1 import SecretStr\n\nfrom langflow.base.models.model import LCModelComponent\nfrom langflow.base.models.openai_constants import OPENAI_MODEL_NAMES\nfrom langflow.field_typing import LanguageModel\nfrom langflow.field_typing.range_spec import RangeSpec\nfrom langflow.inputs import BoolInput, DictInput, DropdownInput, FloatInput, IntInput, SecretStrInput, StrInput\nfrom langflow.inputs.inputs import HandleInput\n\n\nclass OpenAIModelComponent(LCModelComponent):\n    display_name = \"OpenAI\"\n    description = \"Generates text using OpenAI LLMs.\"\n    icon = \"OpenAI\"\n    name = \"OpenAIModel\"\n\n    inputs = [\n        *LCModelComponent._base_inputs,\n        IntInput(\n            name=\"max_tokens\",\n            display_name=\"Max Tokens\",\n            advanced=True,\n            info=\"The maximum number of tokens to generate. Set to 0 for unlimited tokens.\",\n            range_spec=RangeSpec(min=0, max=128000),\n        ),\n        DictInput(\n            name=\"model_kwargs\",\n            display_name=\"Model Kwargs\",\n            advanced=True,\n            info=\"Additional keyword arguments to pass to the model.\",\n        ),\n        BoolInput(\n            name=\"json_mode\",\n            display_name=\"JSON Mode\",\n            advanced=True,\n            info=\"If True, it will output JSON regardless of passing a schema.\",\n        ),\n        DictInput(\n            name=\"output_schema\",\n            is_list=True,\n            display_name=\"Schema\",\n            advanced=True,\n            info=\"The schema for the Output of the model. \"\n            \"You must pass the word JSON in the prompt. \"\n            \"If left blank, JSON mode will be disabled. [DEPRECATED]\",\n        ),\n        DropdownInput(\n            name=\"model_name\",\n            display_name=\"Model Name\",\n            advanced=False,\n            options=OPENAI_MODEL_NAMES,\n            value=OPENAI_MODEL_NAMES[0],\n        ),\n        StrInput(\n            name=\"openai_api_base\",\n            display_name=\"OpenAI API Base\",\n            advanced=True,\n            info=\"The base URL of the OpenAI API. \"\n            \"Defaults to https://api.openai.com/v1. \"\n            \"You can change this to use other APIs like JinaChat, LocalAI and Prem.\",\n        ),\n        SecretStrInput(\n            name=\"api_key\",\n            display_name=\"OpenAI API Key\",\n            info=\"The OpenAI API Key to use for the OpenAI model.\",\n            advanced=False,\n            value=\"OPENAI_API_KEY\",\n        ),\n        FloatInput(name=\"temperature\", display_name=\"Temperature\", value=0.1),\n        IntInput(\n            name=\"seed\",\n            display_name=\"Seed\",\n            info=\"The seed controls the reproducibility of the job.\",\n            advanced=True,\n            value=1,\n        ),\n        HandleInput(\n            name=\"output_parser\",\n            display_name=\"Output Parser\",\n            info=\"The parser to use to parse the output of the model\",\n            advanced=True,\n            input_types=[\"OutputParser\"],\n        ),\n    ]\n\n    def build_model(self) -> LanguageModel:  # type: ignore[type-var]\n        # self.output_schema is a list of dictionaries\n        # let's convert it to a dictionary\n        output_schema_dict: dict[str, str] = reduce(operator.ior, self.output_schema or {}, {})\n        openai_api_key = self.api_key\n        temperature = self.temperature\n        model_name: str = self.model_name\n        max_tokens = self.max_tokens\n        model_kwargs = self.model_kwargs or {}\n        openai_api_base = self.openai_api_base or \"https://api.openai.com/v1\"\n        json_mode = bool(output_schema_dict) or self.json_mode\n        seed = self.seed\n\n        api_key = SecretStr(openai_api_key).get_secret_value() if openai_api_key else None\n        output = ChatOpenAI(\n            max_tokens=max_tokens or None,\n            model_kwargs=model_kwargs,\n            model=model_name,\n            base_url=openai_api_base,\n            api_key=api_key,\n            temperature=temperature if temperature is not None else 0.1,\n            seed=seed,\n        )\n        if json_mode:\n            if output_schema_dict:\n                output = output.with_structured_output(schema=output_schema_dict, method=\"json_mode\")\n            else:\n                output = output.bind(response_format={\"type\": \"json_object\"})\n\n        return output\n\n    def _get_exception_message(self, e: Exception):\n        \"\"\"Get a message from an OpenAI exception.\n\n        Args:\n            e (Exception): The exception to get the message from.\n\n        Returns:\n            str: The message from the exception.\n        \"\"\"\n        try:\n            from openai import BadRequestError\n        except ImportError:\n            return None\n        if isinstance(e, BadRequestError):\n            message = e.body.get(\"message\")\n            if message:\n                return message\n        return None\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false},"input_value":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"input_value","value":"","display_name":"Input","advanced":false,"input_types":["Message"],"dynamic":false,"info":"","title_case":false,"type":"str","_input_type":"MessageInput"},"json_mode":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"json_mode","value":false,"display_name":"JSON Mode","advanced":true,"dynamic":false,"info":"If True, it will output JSON regardless of passing a schema.","title_case":false,"type":"bool","_input_type":"BoolInput"},"max_tokens":{"trace_as_metadata":true,"range_spec":{"step_type":"float","min":0,"max":128000,"step":0.1},"list":false,"required":false,"placeholder":"","show":true,"name":"max_tokens","value":"","display_name":"Max Tokens","advanced":true,"dynamic":false,"info":"The maximum number of tokens to generate. Set to 0 for unlimited tokens.","title_case":false,"type":"int","_input_type":"IntInput"},"model_kwargs":{"trace_as_input":true,"list":false,"required":false,"placeholder":"","show":true,"name":"model_kwargs","value":{},"display_name":"Model Kwargs","advanced":true,"dynamic":false,"info":"Additional keyword arguments to pass to the model.","title_case":false,"type":"dict","_input_type":"DictInput"},"model_name":{"tool_mode":false,"trace_as_metadata":true,"options":["gpt-4o-mini","gpt-4o","gpt-4-turbo","gpt-4-turbo-preview","gpt-4","gpt-3.5-turbo","gpt-3.5-turbo-0125"],"combobox":false,"required":false,"placeholder":"","show":true,"name":"model_name","value":"gpt-4o","display_name":"Model Name","advanced":false,"dynamic":false,"info":"","title_case":false,"type":"str","_input_type":"DropdownInput","load_from_db":false},"openai_api_base":{"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"openai_api_base","value":"","display_name":"OpenAI API Base","advanced":true,"dynamic":false,"info":"The base URL of the OpenAI API. Defaults to https://api.openai.com/v1. You can change this to use other APIs like JinaChat, LocalAI and Prem.","title_case":false,"type":"str","_input_type":"StrInput"},"output_schema":{"trace_as_input":true,"list":true,"required":false,"placeholder":"","show":true,"name":"output_schema","value":{},"display_name":"Schema","advanced":true,"dynamic":false,"info":"The schema for the Output of the model. You must pass the word JSON in the prompt. If left blank, JSON mode will be disabled. [DEPRECATED]","title_case":false,"type":"dict","_input_type":"DictInput"},"seed":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"seed","value":1,"display_name":"Seed","advanced":true,"dynamic":false,"info":"The seed controls the reproducibility of the job.","title_case":false,"type":"int","_input_type":"IntInput"},"stream":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"stream","value":false,"display_name":"Stream","advanced":false,"dynamic":false,"info":"Stream the response from the model. Streaming works only in Chat.","title_case":false,"type":"bool","_input_type":"BoolInput"},"system_message":{"tool_mode":false,"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"system_message","value":"","display_name":"System Message","advanced":false,"input_types":["Message"],"dynamic":false,"info":"System message to pass to the model.","title_case":false,"type":"str","_input_type":"MessageTextInput"},"temperature":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"temperature","value":0,"display_name":"Temperature","advanced":false,"dynamic":false,"info":"","title_case":false,"type":"float","_input_type":"FloatInput","load_from_db":false}},"description":"Generates text using OpenAI LLMs.","icon":"OpenAI","base_classes":["LanguageModel","Message"],"display_name":"OpenAI","documentation":"","custom_fields":{},"output_types":[],"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Message"],"selected":"Message","name":"text_output","display_name":"Text","method":"text_response","value":"__UNDEFINED__","cache":true,"required_inputs":[]},{"types":["LanguageModel"],"selected":"LanguageModel","name":"model_output","display_name":"Language Model","method":"build_model","value":"__UNDEFINED__","cache":true,"required_inputs":[]}],"field_order":["input_value","system_message","stream","max_tokens","model_kwargs","json_mode","output_schema","model_name","openai_api_base","api_key","temperature","seed","output_parser"],"beta":false,"legacy":false,"edited":false,"metadata":{},"tool_mode":false,"lf_version":"1.1.0"},"id":"OpenAIModel-wT8PD","description":"Generates text using OpenAI LLMs.","display_name":"OpenAI"},"selected":false,"width":320,"height":671,"positionAbsolute":{"x":2346.2008802910887,"y":521.0123692316673},"dragging":false},{"id":"Prompt-HMpF4","type":"genericNode","position":{"x":2128.755014040892,"y":1351.4158742028903},"data":{"type":"Prompt","node":{"template":{"_type":"Component","code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"from langflow.base.prompts.api_utils import process_prompt_template\nfrom langflow.custom import Component\nfrom langflow.inputs.inputs import DefaultPromptField\nfrom langflow.io import Output, PromptInput\nfrom langflow.schema.message import Message\nfrom langflow.template.utils import update_template_values\n\n\nclass PromptComponent(Component):\n    display_name: str = \"Prompt\"\n    description: str = \"Create a prompt template with dynamic variables.\"\n    icon = \"prompts\"\n    trace_type = \"prompt\"\n    name = \"Prompt\"\n\n    inputs = [\n        PromptInput(name=\"template\", display_name=\"Template\"),\n    ]\n\n    outputs = [\n        Output(display_name=\"Prompt Message\", name=\"prompt\", method=\"build_prompt\"),\n    ]\n\n    async def build_prompt(self) -> Message:\n        prompt = Message.from_template(**self._attributes)\n        self.status = prompt.text\n        return prompt\n\n    def _update_template(self, frontend_node: dict):\n        prompt_template = frontend_node[\"template\"][\"template\"][\"value\"]\n        custom_fields = frontend_node[\"custom_fields\"]\n        frontend_node_template = frontend_node[\"template\"]\n        _ = process_prompt_template(\n            template=prompt_template,\n            name=\"template\",\n            custom_fields=custom_fields,\n            frontend_node_template=frontend_node_template,\n        )\n        return frontend_node\n\n    def post_code_processing(self, new_frontend_node: dict, current_frontend_node: dict):\n        \"\"\"This function is called after the code validation is done.\"\"\"\n        frontend_node = super().post_code_processing(new_frontend_node, current_frontend_node)\n        template = frontend_node[\"template\"][\"template\"][\"value\"]\n        # Kept it duplicated for backwards compatibility\n        _ = process_prompt_template(\n            template=template,\n            name=\"template\",\n            custom_fields=frontend_node[\"custom_fields\"],\n            frontend_node_template=frontend_node[\"template\"],\n        )\n        # Now that template is updated, we need to grab any values that were set in the current_frontend_node\n        # and update the frontend_node with those values\n        update_template_values(new_template=frontend_node, previous_template=current_frontend_node[\"template\"])\n        return frontend_node\n\n    def _get_fallback_input(self, **kwargs):\n        return DefaultPromptField(**kwargs)\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false},"template":{"tool_mode":false,"trace_as_input":true,"list":false,"required":false,"placeholder":"","show":true,"name":"template","value":"You are a principal software engineer reviewing the work of another engineer. You will provide an honest review of their work, outlining mistakes they made, if any. If they made no mistakes, respond \"LGTM\".\nYou will review their task description and their output and ensure they followed their task correctly.\n\nTHEIR TASK:\n---\n{task}\n---\n\nTHEIR OUTPUT:\n---\n{output}\n---\n","display_name":"Template","advanced":false,"dynamic":false,"info":"","title_case":false,"type":"prompt","_input_type":"PromptInput","load_from_db":false},"task":{"field_type":"str","required":false,"placeholder":"","list":false,"show":true,"multiline":true,"value":"","fileTypes":[],"file_path":"","name":"task","display_name":"task","advanced":false,"input_types":["Message","Text"],"dynamic":false,"info":"","load_from_db":false,"title_case":false,"type":"str"},"output":{"field_type":"str","required":false,"placeholder":"","list":false,"show":true,"multiline":true,"value":"","fileTypes":[],"file_path":"","name":"output","display_name":"output","advanced":false,"input_types":["Message","Text"],"dynamic":false,"info":"","load_from_db":false,"title_case":false,"type":"str"}},"description":"Create a prompt template with dynamic variables.","icon":"prompts","is_input":null,"is_output":null,"is_composition":null,"base_classes":["Message"],"name":"","display_name":"Reviewer Prompt ","documentation":"","custom_fields":{"template":["task","output"]},"output_types":[],"full_path":null,"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Message"],"selected":"Message","name":"prompt","hidden":null,"display_name":"Prompt Message","method":"build_prompt","value":"__UNDEFINED__","cache":true,"required_inputs":null}],"field_order":["template"],"beta":false,"legacy":false,"error":null,"edited":false,"metadata":{},"tool_mode":false,"lf_version":"1.1.0"},"id":"Prompt-HMpF4","description":"Create a prompt template with dynamic variables.","display_name":"Reviewer Prompt "},"selected":false,"width":320,"height":431,"positionAbsolute":{"x":2128.755014040892,"y":1351.4158742028903},"dragging":false},{"id":"OpenAIModel-cfLdA","type":"genericNode","position":{"x":2592.7536735196577,"y":1392.5080064076517},"data":{"type":"OpenAIModel","node":{"template":{"_type":"Component","output_parser":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"output_parser","value":"","display_name":"Output Parser","advanced":true,"input_types":["OutputParser"],"dynamic":false,"info":"The parser to use to parse the output of the model","title_case":false,"type":"other","_input_type":"HandleInput"},"api_key":{"load_from_db":true,"required":false,"placeholder":"","show":true,"name":"api_key","value":"OPENAI_API_KEY","display_name":"OpenAI API Key","advanced":false,"input_types":["Message"],"dynamic":false,"info":"The OpenAI API Key to use for the OpenAI model.","title_case":false,"password":true,"type":"str","_input_type":"SecretStrInput"},"code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"import operator\nfrom functools import reduce\n\nfrom langchain_openai import ChatOpenAI\nfrom pydantic.v1 import SecretStr\n\nfrom langflow.base.models.model import LCModelComponent\nfrom langflow.base.models.openai_constants import OPENAI_MODEL_NAMES\nfrom langflow.field_typing import LanguageModel\nfrom langflow.field_typing.range_spec import RangeSpec\nfrom langflow.inputs import BoolInput, DictInput, DropdownInput, FloatInput, IntInput, SecretStrInput, StrInput\nfrom langflow.inputs.inputs import HandleInput\n\n\nclass OpenAIModelComponent(LCModelComponent):\n    display_name = \"OpenAI\"\n    description = \"Generates text using OpenAI LLMs.\"\n    icon = \"OpenAI\"\n    name = \"OpenAIModel\"\n\n    inputs = [\n        *LCModelComponent._base_inputs,\n        IntInput(\n            name=\"max_tokens\",\n            display_name=\"Max Tokens\",\n            advanced=True,\n            info=\"The maximum number of tokens to generate. Set to 0 for unlimited tokens.\",\n            range_spec=RangeSpec(min=0, max=128000),\n        ),\n        DictInput(\n            name=\"model_kwargs\",\n            display_name=\"Model Kwargs\",\n            advanced=True,\n            info=\"Additional keyword arguments to pass to the model.\",\n        ),\n        BoolInput(\n            name=\"json_mode\",\n            display_name=\"JSON Mode\",\n            advanced=True,\n            info=\"If True, it will output JSON regardless of passing a schema.\",\n        ),\n        DictInput(\n            name=\"output_schema\",\n            is_list=True,\n            display_name=\"Schema\",\n            advanced=True,\n            info=\"The schema for the Output of the model. \"\n            \"You must pass the word JSON in the prompt. \"\n            \"If left blank, JSON mode will be disabled. [DEPRECATED]\",\n        ),\n        DropdownInput(\n            name=\"model_name\",\n            display_name=\"Model Name\",\n            advanced=False,\n            options=OPENAI_MODEL_NAMES,\n            value=OPENAI_MODEL_NAMES[0],\n        ),\n        StrInput(\n            name=\"openai_api_base\",\n            display_name=\"OpenAI API Base\",\n            advanced=True,\n            info=\"The base URL of the OpenAI API. \"\n            \"Defaults to https://api.openai.com/v1. \"\n            \"You can change this to use other APIs like JinaChat, LocalAI and Prem.\",\n        ),\n        SecretStrInput(\n            name=\"api_key\",\n            display_name=\"OpenAI API Key\",\n            info=\"The OpenAI API Key to use for the OpenAI model.\",\n            advanced=False,\n            value=\"OPENAI_API_KEY\",\n        ),\n        FloatInput(name=\"temperature\", display_name=\"Temperature\", value=0.1),\n        IntInput(\n            name=\"seed\",\n            display_name=\"Seed\",\n            info=\"The seed controls the reproducibility of the job.\",\n            advanced=True,\n            value=1,\n        ),\n        HandleInput(\n            name=\"output_parser\",\n            display_name=\"Output Parser\",\n            info=\"The parser to use to parse the output of the model\",\n            advanced=True,\n            input_types=[\"OutputParser\"],\n        ),\n    ]\n\n    def build_model(self) -> LanguageModel:  # type: ignore[type-var]\n        # self.output_schema is a list of dictionaries\n        # let's convert it to a dictionary\n        output_schema_dict: dict[str, str] = reduce(operator.ior, self.output_schema or {}, {})\n        openai_api_key = self.api_key\n        temperature = self.temperature\n        model_name: str = self.model_name\n        max_tokens = self.max_tokens\n        model_kwargs = self.model_kwargs or {}\n        openai_api_base = self.openai_api_base or \"https://api.openai.com/v1\"\n        json_mode = bool(output_schema_dict) or self.json_mode\n        seed = self.seed\n\n        api_key = SecretStr(openai_api_key).get_secret_value() if openai_api_key else None\n        output = ChatOpenAI(\n            max_tokens=max_tokens or None,\n            model_kwargs=model_kwargs,\n            model=model_name,\n            base_url=openai_api_base,\n            api_key=api_key,\n            temperature=temperature if temperature is not None else 0.1,\n            seed=seed,\n        )\n        if json_mode:\n            if output_schema_dict:\n                output = output.with_structured_output(schema=output_schema_dict, method=\"json_mode\")\n            else:\n                output = output.bind(response_format={\"type\": \"json_object\"})\n\n        return output\n\n    def _get_exception_message(self, e: Exception):\n        \"\"\"Get a message from an OpenAI exception.\n\n        Args:\n            e (Exception): The exception to get the message from.\n\n        Returns:\n            str: The message from the exception.\n        \"\"\"\n        try:\n            from openai import BadRequestError\n        except ImportError:\n            return None\n        if isinstance(e, BadRequestError):\n            message = e.body.get(\"message\")\n            if message:\n                return message\n        return None\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false},"input_value":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"input_value","value":"","display_name":"Input","advanced":false,"input_types":["Message"],"dynamic":false,"info":"","title_case":false,"type":"str","_input_type":"MessageInput"},"json_mode":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"json_mode","value":false,"display_name":"JSON Mode","advanced":true,"dynamic":false,"info":"If True, it will output JSON regardless of passing a schema.","title_case":false,"type":"bool","_input_type":"BoolInput"},"max_tokens":{"trace_as_metadata":true,"range_spec":{"step_type":"float","min":0,"max":128000,"step":0.1},"list":false,"required":false,"placeholder":"","show":true,"name":"max_tokens","value":"","display_name":"Max Tokens","advanced":true,"dynamic":false,"info":"The maximum number of tokens to generate. Set to 0 for unlimited tokens.","title_case":false,"type":"int","_input_type":"IntInput"},"model_kwargs":{"trace_as_input":true,"list":false,"required":false,"placeholder":"","show":true,"name":"model_kwargs","value":{},"display_name":"Model Kwargs","advanced":true,"dynamic":false,"info":"Additional keyword arguments to pass to the model.","title_case":false,"type":"dict","_input_type":"DictInput"},"model_name":{"tool_mode":false,"trace_as_metadata":true,"options":["gpt-4o-mini","gpt-4o","gpt-4-turbo","gpt-4-turbo-preview","gpt-4","gpt-3.5-turbo","gpt-3.5-turbo-0125"],"combobox":false,"required":false,"placeholder":"","show":true,"name":"model_name","value":"gpt-4o","display_name":"Model Name","advanced":false,"dynamic":false,"info":"","title_case":false,"type":"str","_input_type":"DropdownInput","load_from_db":false},"openai_api_base":{"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"openai_api_base","value":"","display_name":"OpenAI API Base","advanced":true,"dynamic":false,"info":"The base URL of the OpenAI API. Defaults to https://api.openai.com/v1. You can change this to use other APIs like JinaChat, LocalAI and Prem.","title_case":false,"type":"str","_input_type":"StrInput"},"output_schema":{"trace_as_input":true,"list":true,"required":false,"placeholder":"","show":true,"name":"output_schema","value":{},"display_name":"Schema","advanced":true,"dynamic":false,"info":"The schema for the Output of the model. You must pass the word JSON in the prompt. If left blank, JSON mode will be disabled. [DEPRECATED]","title_case":false,"type":"dict","_input_type":"DictInput"},"seed":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"seed","value":1,"display_name":"Seed","advanced":true,"dynamic":false,"info":"The seed controls the reproducibility of the job.","title_case":false,"type":"int","_input_type":"IntInput"},"stream":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"stream","value":false,"display_name":"Stream","advanced":false,"dynamic":false,"info":"Stream the response from the model. Streaming works only in Chat.","title_case":false,"type":"bool","_input_type":"BoolInput"},"system_message":{"tool_mode":false,"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"system_message","value":"","display_name":"System Message","advanced":false,"input_types":["Message"],"dynamic":false,"info":"System message to pass to the model.","title_case":false,"type":"str","_input_type":"MessageTextInput"},"temperature":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"temperature","value":0,"display_name":"Temperature","advanced":false,"dynamic":false,"info":"","title_case":false,"type":"float","_input_type":"FloatInput","load_from_db":false}},"description":"Generates text using OpenAI LLMs.","icon":"OpenAI","base_classes":["LanguageModel","Message"],"display_name":"OpenAI","documentation":"","custom_fields":{},"output_types":[],"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Message"],"selected":"Message","name":"text_output","display_name":"Text","method":"text_response","value":"__UNDEFINED__","cache":true,"required_inputs":[]},{"types":["LanguageModel"],"selected":"LanguageModel","name":"model_output","display_name":"Language Model","method":"build_model","value":"__UNDEFINED__","cache":true,"required_inputs":[]}],"field_order":["input_value","system_message","stream","max_tokens","model_kwargs","json_mode","output_schema","model_name","openai_api_base","api_key","temperature","seed","output_parser"],"beta":false,"legacy":false,"edited":false,"metadata":{},"tool_mode":false,"lf_version":"1.1.0"},"id":"OpenAIModel-cfLdA","description":"Generates text using OpenAI LLMs.","display_name":"OpenAI"},"selected":false,"width":320,"height":671,"positionAbsolute":{"x":2592.7536735196577,"y":1392.5080064076517},"dragging":false},{"id":"TextInput-J2Cqw","type":"genericNode","position":{"x":1390.798187001297,"y":249.52141220756113},"data":{"node":{"template":{"_type":"Component","code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"from langflow.base.io.text import TextComponent\nfrom langflow.io import MultilineInput, Output\nfrom langflow.schema.message import Message\n\n\nclass TextInputComponent(TextComponent):\n    display_name = \"Text Input\"\n    description = \"Get text inputs from the Playground.\"\n    icon = \"type\"\n    name = \"TextInput\"\n\n    inputs = [\n        MultilineInput(\n            name=\"input_value\",\n            display_name=\"Text\",\n            info=\"Text to be passed as input.\",\n        ),\n    ]\n    outputs = [\n        Output(display_name=\"Text\", name=\"text\", method=\"text_response\"),\n    ]\n\n    def text_response(self) -> Message:\n        return Message(\n            text=self.input_value,\n        )\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false},"input_value":{"tool_mode":false,"trace_as_input":true,"multiline":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"input_value","value":"'use strict';  Blockly.JSON.toWorkspace = function(json_text, workspace) {     var json_structure  = JSON.parse(json_text);     workspace.clear();     var startBlock = Blockly.Block.obtain(workspace, 'start');     startBlock.initSvg();     startBlock.render();     Blockly.JSON.buildAndConnect(json_structure, startBlock.getInput('json').connection); };  Blockly.JSON.buildAndConnect = function(json_structure, parentConnection) {     if(json_structure === null) {         return;     } else {         var json_keys = Object.keys(json_structure);         var type  = typeof(json_structure);         var json_values = Object.values(json_structure);         if(type == 'boolean') {             type = \"true_false\";         }          else if(type == 'object') {              if (json_keys[0] == \"if\"){                 type = \"if_logic\";             }else if(json_keys[0] == \"var\"){                 type = \"var\";             }else if(json_keys[0] == \"!=\" || json_keys[0] == \"===\" || json_keys[0] == \"==\"                      || json_keys[0] == \"!==\"){                 type = \"logical\";             }else if(json_keys[0] == \"!\" || json_keys[0] == \"!!\"){                 type = \"not\";             }else if(json_keys[0] == \"and\" || json_keys[0] == \"or\"){                 type = \"boolean\";             }else if(json_keys[0] == \"min\" || json_keys[0] == \"max\"){                 type = \"minmax\";             }else if(json_keys[0] == \"*\" || json_keys[0] == \"/\" || json_keys[0] == \"+\"                      || json_keys[0] == \"-\" || json_keys[0] == \"%\"){                 type = \"arithmatic\";             }else if(json_keys[0] == \"map\" || json_keys[0] == \"reduce\" || json_keys[0] == \"filter\"                      || json_keys[0] == \"all\" || json_keys[0] == \"none\" || json_keys[0] == \"some\"){                 type = \"map_filter\";             }else if(json_keys[0] == \"merge\"){                 type = \"merge\";             }else if (json_keys[0] == \">\" || json_keys[0] == \">=\" || json_keys[0] == \"<\"                      || json_keys[0] == \"<=\"){                 var json_values = Object.values(json_structure);                 if (json_values[0].length == 3){                     type = \"between\";                 }else{                      type = \"comparison\";                 }             }else if (json_keys[0] == \"in\"){                 var json_values = Object.values(json_structure);                 type = (json_values[0][1] instanceof Array) ? \"InMiss\" : \"inString\";             }             else if(json_keys[0] == \"missing\" || json_keys[0] == \"missing_some\"){                 type = \"InMiss\";             }else if(json_keys[0] == \"cat\"){                 type = \"catString\";             }else if(json_keys[0] == \"substr\"){                 type = \"subStr\";             }else {                 type = (json_structure instanceof Array) ? 'array' : 'dictionary';             }         }          var targetBlock = Blockly.Block.obtain(parentConnection.sourceBlock_.workspace, type);         targetBlock.initSvg();         targetBlock.render();          var childConnection = targetBlock.outputConnection;         parentConnection.connect(childConnection);          switch(type) {              case 'string':                 targetBlock.setFieldValue( String(json_structure), 'string_value' );                 break;             case 'number':                 targetBlock.setFieldValue( String(json_structure), 'number_value' );                 break;             case 'true_false':                 targetBlock.setFieldValue(String(Boolean(json_structure)), 'bool');                 break;             case 'var':                 var i=0;                 for(var key in json_structure) {                     var elementConnection = targetBlock.getInput('json'+i).connection;                     Blockly.JSON.buildAndConnect(json_structure[key], elementConnection);                     i++;                 }                 break;             case 'dictionary':                 var i=0;                 for(var key in json_structure) {                     targetBlock.appendKeyValuePairInput();                     targetBlock.setFieldValue( key, 'key_field_'+i );                     var elementConnection = targetBlock.getInput('element_'+i).connection;                     Blockly.JSON.buildAndConnect(json_structure[key], elementConnection);                     i++;                 }                 break;             case 'array':                 for(var i in json_structure) {                     targetBlock.appendElementInput();                     var elementConnection = targetBlock.getInput('element_'+i).connection;                     Blockly.JSON.buildAndConnect(json_structure[i], elementConnection);                 }                 break;             case 'if_logic':                 var key = json_keys[0];                 for(var i in json_structure[key]) {                     targetBlock.appendElementInput();                     var elementConnection = targetBlock.getInput('element_'+i).connection;                     Blockly.JSON.buildAndConnect(json_structure[key][i], elementConnection);                 }                 break;             case 'logical':                 var key = json_keys[0];                  for(var i in json_structure[key]) {                     targetBlock.setFieldValue( key, 'operator');                     var elementConnection = targetBlock.getInput('json'+i).connection;                     Blockly.JSON.buildAndConnect(json_structure[key][i], elementConnection);                 }                 break;             case 'not':                 var key = json_keys[0];                  for(var i in json_structure[key]) {                     targetBlock.setFieldValue( key, 'operator');                     var elementConnection = targetBlock.getInput('json'+i).connection;                     Blockly.JSON.buildAndConnect(json_structure[key][i], elementConnection);                 }                 break;             case 'boolean':                 var key=json_keys[0];                 for(var i in json_structure[key]) {                     targetBlock.appendElementInput();                     targetBlock.setFieldValue( key, 'operator');                     var elementConnection = targetBlock.getInput('element_'+i).connection;                     Blockly.JSON.buildAndConnect(json_structure[key][i], elementConnection);                 }                 break;             case 'comparison':                 var key = json_keys[0];                  for(var i in json_structure[key]) {                     targetBlock.setFieldValue( key, 'operator');                     var elementConnection = targetBlock.getInput('json'+i).connection;                     Blockly.JSON.buildAndConnect(json_structure[key][i], elementConnection);                 }                 break;             case 'minmax':                 var key=json_keys[0];                 for(var i in json_structure[key]) {                     targetBlock.appendElementInput();                     targetBlock.setFieldValue( key, 'operator');                     var elementConnection = targetBlock.getInput('element_'+i).connection;                     Blockly.JSON.buildAndConnect(json_structure[key][i], elementConnection);                 }                 break;             case 'between':                 var key = json_keys[0];                  for(var i in json_structure[key]) {                     targetBlock.setFieldValue( key, 'operator');                     var elementConnection = targetBlock.getInput('json'+i).connection;                     Blockly.JSON.buildAndConnect(json_structure[key][i], elementConnection);                 }                 break;             case 'arithmatic':                 var key=json_keys[0];                 for(var i in json_structure[key]) {                     targetBlock.appendElementInput();                     targetBlock.setFieldValue( key, 'operator');                     var elementConnection = targetBlock.getInput('element_'+i).connection;                     Blockly.JSON.buildAndConnect(json_structure[key][i], elementConnection);                 }                 break;             case 'arithmatic':                 var key=json_keys[0];                 for(var i in json_structure[key]) {                     targetBlock.appendElementInput();                     targetBlock.setFieldValue( key, 'operator');                     var elementConnection = targetBlock.getInput('element_'+i).connection;                     Blockly.JSON.buildAndConnect(json_structure[key][i], elementConnection);                 }                 break;             case 'map_filter':                 var key=json_keys[0];                 for(var i in json_structure[key]) {                     targetBlock.appendElementInput();                     targetBlock.setFieldValue( key, 'operator');                     var elementConnection = targetBlock.getInput('element_'+i).connection;                     Blockly.JSON.buildAndConnect(json_structure[key][i], elementConnection);                 }                 break;             case 'merge':                 var key = json_keys[0];                 for(var i in json_structure[key]) {                     targetBlock.appendElementInput();                     var elementConnection = targetBlock.getInput('element_'+i).connection;                     Blockly.JSON.buildAndConnect(json_structure[key][i], elementConnection);                 }                 break;             case 'InMiss':                 var key=json_keys[0];                 for(var i in json_structure[key]) {                     targetBlock.appendElementInput();                     targetBlock.setFieldValue( key, 'operator');                     var elementConnection = targetBlock.getInput('element_'+i).connection;                     Blockly.JSON.buildAndConnect(json_structure[key][i], elementConnection);                 }                 break;             case 'inString':                 var key=json_keys[0];                 for(var i in json_structure[key]) {                     var elementConnection = targetBlock.getInput('json'+i).connection;                     Blockly.JSON.buildAndConnect(json_structure[key][i], elementConnection);                 }                 break;             case 'catString':                 var key = json_keys[0];                 for(var i in json_structure[key]) {                     targetBlock.appendElementInput();                     var elementConnection = targetBlock.getInput('element_'+i).connection;                     Blockly.JSON.buildAndConnect(json_structure[key][i], elementConnection);                 }                 break;             case 'subStr':                 var key = json_keys[0];                 for(var i in json_structure[key]) {                     targetBlock.appendElementInput();                     var elementConnection = targetBlock.getInput('element_'+i).connection;                     Blockly.JSON.buildAndConnect(json_structure[key][i], elementConnection);                 }                 break;         }     } };","display_name":"Text","advanced":false,"input_types":["Message"],"dynamic":false,"info":"Text to be passed as input.","title_case":false,"type":"str","_input_type":"MultilineInput"}},"description":"Get text inputs from the Playground.","icon":"type","base_classes":["Message"],"display_name":"JsonLogic code","documentation":"","custom_fields":{},"output_types":[],"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Message"],"selected":"Message","name":"text","display_name":"Text","method":"text_response","value":"__UNDEFINED__","cache":true}],"field_order":["input_value"],"beta":false,"legacy":false,"edited":false,"metadata":{},"tool_mode":false,"lf_version":"1.1.0"},"type":"TextInput","id":"TextInput-J2Cqw"},"selected":true,"width":320,"height":233,"dragging":false,"positionAbsolute":{"x":1390.798187001297,"y":249.52141220756113}}],"edges":[{"source":"Prompt-A7lUJ","target":"OpenAIModel-nQX38","sourceHandle":"{œdataTypeœ:œPromptœ,œidœ:œPrompt-A7lUJœ,œnameœ:œpromptœ,œoutput_typesœ:[œMessageœ]}","targetHandle":"{œfieldNameœ:œinput_valueœ,œidœ:œOpenAIModel-nQX38œ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}","id":"reactflow__edge-Prompt-A7lUJ{œdataTypeœ:œPromptœ,œidœ:œPrompt-A7lUJœ,œnameœ:œpromptœ,œoutput_typesœ:[œMessageœ]}-OpenAIModel-nQX38{œfieldNameœ:œinput_valueœ,œidœ:œOpenAIModel-nQX38œ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}","data":{"targetHandle":{"fieldName":"input_value","id":"OpenAIModel-nQX38","inputTypes":["Message"],"type":"str"},"sourceHandle":{"dataType":"Prompt","id":"Prompt-A7lUJ","name":"prompt","output_types":["Message"]}},"selected":false,"animated":false,"className":""},{"source":"Prompt-7DbwR","target":"OpenAIModel-scdKD","sourceHandle":"{œdataTypeœ:œPromptœ,œidœ:œPrompt-7DbwRœ,œnameœ:œpromptœ,œoutput_typesœ:[œMessageœ]}","targetHandle":"{œfieldNameœ:œinput_valueœ,œidœ:œOpenAIModel-scdKDœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}","id":"reactflow__edge-Prompt-7DbwR{œdataTypeœ:œPromptœ,œidœ:œPrompt-7DbwRœ,œnameœ:œpromptœ,œoutput_typesœ:[œMessageœ]}-OpenAIModel-scdKD{œfieldNameœ:œinput_valueœ,œidœ:œOpenAIModel-scdKDœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}","data":{"targetHandle":{"fieldName":"input_value","id":"OpenAIModel-scdKD","inputTypes":["Message"],"type":"str"},"sourceHandle":{"dataType":"Prompt","id":"Prompt-7DbwR","name":"prompt","output_types":["Message"]}},"selected":false,"animated":false,"className":""},{"source":"URL-lCw7J","sourceHandle":"{œdataTypeœ:œURLœ,œidœ:œURL-lCw7Jœ,œnameœ:œtextœ,œoutput_typesœ:[œMessageœ]}","target":"Prompt-A7lUJ","targetHandle":"{œfieldNameœ:œdocumentationœ,œidœ:œPrompt-A7lUJœ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}","data":{"targetHandle":{"fieldName":"documentation","id":"Prompt-A7lUJ","inputTypes":["Message","Text"],"type":"str"},"sourceHandle":{"dataType":"URL","id":"URL-lCw7J","name":"text","output_types":["Message"]}},"id":"reactflow__edge-URL-lCw7J{œdataTypeœ:œURLœ,œidœ:œURL-lCw7Jœ,œnameœ:œtextœ,œoutput_typesœ:[œMessageœ]}-Prompt-A7lUJ{œfieldNameœ:œdocumentationœ,œidœ:œPrompt-A7lUJœ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}","animated":false,"className":""},{"source":"TextInput-X22Qw","sourceHandle":"{œdataTypeœ:œTextInputœ,œidœ:œTextInput-X22Qwœ,œnameœ:œtextœ,œoutput_typesœ:[œMessageœ]}","target":"Prompt-A7lUJ","targetHandle":"{œfieldNameœ:œlibraryœ,œidœ:œPrompt-A7lUJœ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}","data":{"targetHandle":{"fieldName":"library","id":"Prompt-A7lUJ","inputTypes":["Message","Text"],"type":"str"},"sourceHandle":{"dataType":"TextInput","id":"TextInput-X22Qw","name":"text","output_types":["Message"]}},"id":"reactflow__edge-TextInput-X22Qw{œdataTypeœ:œTextInputœ,œidœ:œTextInput-X22Qwœ,œnameœ:œtextœ,œoutput_typesœ:[œMessageœ]}-Prompt-A7lUJ{œfieldNameœ:œlibraryœ,œidœ:œPrompt-A7lUJœ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}","animated":false,"className":""},{"source":"Prompt-A7lUJ","sourceHandle":"{œdataTypeœ:œPromptœ,œidœ:œPrompt-A7lUJœ,œnameœ:œpromptœ,œoutput_typesœ:[œMessageœ]}","target":"Prompt-7DbwR","targetHandle":"{œfieldNameœ:œtaskœ,œidœ:œPrompt-7DbwRœ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}","data":{"targetHandle":{"fieldName":"task","id":"Prompt-7DbwR","inputTypes":["Message","Text"],"type":"str"},"sourceHandle":{"dataType":"Prompt","id":"Prompt-A7lUJ","name":"prompt","output_types":["Message"]}},"id":"reactflow__edge-Prompt-A7lUJ{œdataTypeœ:œPromptœ,œidœ:œPrompt-A7lUJœ,œnameœ:œpromptœ,œoutput_typesœ:[œMessageœ]}-Prompt-7DbwR{œfieldNameœ:œtaskœ,œidœ:œPrompt-7DbwRœ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}","animated":false,"className":""},{"source":"OpenAIModel-nQX38","sourceHandle":"{œdataTypeœ:œOpenAIModelœ,œidœ:œOpenAIModel-nQX38œ,œnameœ:œtext_outputœ,œoutput_typesœ:[œMessageœ]}","target":"Prompt-7DbwR","targetHandle":"{œfieldNameœ:œoutputœ,œidœ:œPrompt-7DbwRœ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}","data":{"targetHandle":{"fieldName":"output","id":"Prompt-7DbwR","inputTypes":["Message","Text"],"type":"str"},"sourceHandle":{"dataType":"OpenAIModel","id":"OpenAIModel-nQX38","name":"text_output","output_types":["Message"]}},"id":"reactflow__edge-OpenAIModel-nQX38{œdataTypeœ:œOpenAIModelœ,œidœ:œOpenAIModel-nQX38œ,œnameœ:œtext_outputœ,œoutput_typesœ:[œMessageœ]}-Prompt-7DbwR{œfieldNameœ:œoutputœ,œidœ:œPrompt-7DbwRœ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}","animated":false,"className":""},{"source":"TextInput-AQXSV","sourceHandle":"{œdataTypeœ:œTextInputœ,œidœ:œTextInput-AQXSVœ,œnameœ:œtextœ,œoutput_typesœ:[œMessageœ]}","target":"Prompt-mNvAI","targetHandle":"{œfieldNameœ:œjson_logic_documentationœ,œidœ:œPrompt-mNvAIœ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}","data":{"targetHandle":{"fieldName":"json_logic_documentation","id":"Prompt-mNvAI","inputTypes":["Message","Text"],"type":"str"},"sourceHandle":{"dataType":"TextInput","id":"TextInput-AQXSV","name":"text","output_types":["Message"]}},"id":"reactflow__edge-TextInput-AQXSV{œdataTypeœ:œTextInputœ,œidœ:œTextInput-AQXSVœ,œnameœ:œtextœ,œoutput_typesœ:[œMessageœ]}-Prompt-mNvAI{œfieldNameœ:œjson_logic_documentationœ,œidœ:œPrompt-mNvAIœ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}","animated":false,"className":""},{"source":"TextInput-7Cjz9","sourceHandle":"{œdataTypeœ:œTextInputœ,œidœ:œTextInput-7Cjz9œ,œnameœ:œtextœ,œoutput_typesœ:[œMessageœ]}","target":"Prompt-mNvAI","targetHandle":"{œfieldNameœ:œblockly_documentationœ,œidœ:œPrompt-mNvAIœ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}","data":{"targetHandle":{"fieldName":"blockly_documentation","id":"Prompt-mNvAI","inputTypes":["Message","Text"],"type":"str"},"sourceHandle":{"dataType":"TextInput","id":"TextInput-7Cjz9","name":"text","output_types":["Message"]}},"id":"reactflow__edge-TextInput-7Cjz9{œdataTypeœ:œTextInputœ,œidœ:œTextInput-7Cjz9œ,œnameœ:œtextœ,œoutput_typesœ:[œMessageœ]}-Prompt-mNvAI{œfieldNameœ:œblockly_documentationœ,œidœ:œPrompt-mNvAIœ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}","animated":false,"className":""},{"source":"Prompt-mNvAI","sourceHandle":"{œdataTypeœ:œPromptœ,œidœ:œPrompt-mNvAIœ,œnameœ:œpromptœ,œoutput_typesœ:[œMessageœ]}","target":"OpenAIModel-wT8PD","targetHandle":"{œfieldNameœ:œinput_valueœ,œidœ:œOpenAIModel-wT8PDœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}","data":{"targetHandle":{"fieldName":"input_value","id":"OpenAIModel-wT8PD","inputTypes":["Message"],"type":"str"},"sourceHandle":{"dataType":"Prompt","id":"Prompt-mNvAI","name":"prompt","output_types":["Message"]}},"id":"reactflow__edge-Prompt-mNvAI{œdataTypeœ:œPromptœ,œidœ:œPrompt-mNvAIœ,œnameœ:œpromptœ,œoutput_typesœ:[œMessageœ]}-OpenAIModel-wT8PD{œfieldNameœ:œinput_valueœ,œidœ:œOpenAIModel-wT8PDœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}","animated":false,"className":""},{"source":"Prompt-mNvAI","sourceHandle":"{œdataTypeœ:œPromptœ,œidœ:œPrompt-mNvAIœ,œnameœ:œpromptœ,œoutput_typesœ:[œMessageœ]}","target":"Prompt-HMpF4","targetHandle":"{œfieldNameœ:œtaskœ,œidœ:œPrompt-HMpF4œ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}","data":{"targetHandle":{"fieldName":"task","id":"Prompt-HMpF4","inputTypes":["Message","Text"],"type":"str"},"sourceHandle":{"dataType":"Prompt","id":"Prompt-mNvAI","name":"prompt","output_types":["Message"]}},"id":"reactflow__edge-Prompt-mNvAI{œdataTypeœ:œPromptœ,œidœ:œPrompt-mNvAIœ,œnameœ:œpromptœ,œoutput_typesœ:[œMessageœ]}-Prompt-HMpF4{œfieldNameœ:œtaskœ,œidœ:œPrompt-HMpF4œ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}","animated":false,"className":""},{"source":"OpenAIModel-wT8PD","sourceHandle":"{œdataTypeœ:œOpenAIModelœ,œidœ:œOpenAIModel-wT8PDœ,œnameœ:œtext_outputœ,œoutput_typesœ:[œMessageœ]}","target":"Prompt-HMpF4","targetHandle":"{œfieldNameœ:œoutputœ,œidœ:œPrompt-HMpF4œ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}","data":{"targetHandle":{"fieldName":"output","id":"Prompt-HMpF4","inputTypes":["Message","Text"],"type":"str"},"sourceHandle":{"dataType":"OpenAIModel","id":"OpenAIModel-wT8PD","name":"text_output","output_types":["Message"]}},"id":"reactflow__edge-OpenAIModel-wT8PD{œdataTypeœ:œOpenAIModelœ,œidœ:œOpenAIModel-wT8PDœ,œnameœ:œtext_outputœ,œoutput_typesœ:[œMessageœ]}-Prompt-HMpF4{œfieldNameœ:œoutputœ,œidœ:œPrompt-HMpF4œ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}","animated":false,"className":""},{"source":"Prompt-HMpF4","sourceHandle":"{œdataTypeœ:œPromptœ,œidœ:œPrompt-HMpF4œ,œnameœ:œpromptœ,œoutput_typesœ:[œMessageœ]}","target":"OpenAIModel-cfLdA","targetHandle":"{œfieldNameœ:œinput_valueœ,œidœ:œOpenAIModel-cfLdAœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}","data":{"targetHandle":{"fieldName":"input_value","id":"OpenAIModel-cfLdA","inputTypes":["Message"],"type":"str"},"sourceHandle":{"dataType":"Prompt","id":"Prompt-HMpF4","name":"prompt","output_types":["Message"]}},"id":"reactflow__edge-Prompt-HMpF4{œdataTypeœ:œPromptœ,œidœ:œPrompt-HMpF4œ,œnameœ:œpromptœ,œoutput_typesœ:[œMessageœ]}-OpenAIModel-cfLdA{œfieldNameœ:œinput_valueœ,œidœ:œOpenAIModel-cfLdAœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}","animated":false,"className":""},{"source":"TextInput-J2Cqw","sourceHandle":"{œdataTypeœ:œTextInputœ,œidœ:œTextInput-J2Cqwœ,œnameœ:œtextœ,œoutput_typesœ:[œMessageœ]}","target":"Prompt-mNvAI","targetHandle":"{œfieldNameœ:œold_codeœ,œidœ:œPrompt-mNvAIœ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}","data":{"targetHandle":{"fieldName":"old_code","id":"Prompt-mNvAI","inputTypes":["Message","Text"],"type":"str"},"sourceHandle":{"dataType":"TextInput","id":"TextInput-J2Cqw","name":"text","output_types":["Message"]}},"id":"reactflow__edge-TextInput-J2Cqw{œdataTypeœ:œTextInputœ,œidœ:œTextInput-J2Cqwœ,œnameœ:œtextœ,œoutput_typesœ:[œMessageœ]}-Prompt-mNvAI{œfieldNameœ:œold_codeœ,œidœ:œPrompt-mNvAIœ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}","animated":false,"className":""}],"viewport":{"x":-264.50002555180913,"y":23.82474622550592,"zoom":0.6254484674875048}},"is_component":false,"updated_at":"2024-11-17T23:15:48+00:00","icon_bg_color":null,"icon":null,"name":"dsl-generator","description":"Given source material, generate a DSL spec","webhook":false,"user_id":"cc587455-d3ae-4830-9b85-e34282e416ad"}