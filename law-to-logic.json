{"name":"Law to Logic","user_id":"b272d732-2068-4af4-b23d-7b825bc4bd8f","endpoint_name":null,"id":"ba1b99b7-4c26-4359-9575-44943c7af815","tags":null,"folder_id":"4c7b9844-9d8e-40f5-b36c-e5e23dcac7ca","gradient":null,"data":{"nodes":[{"id":"URL-6WzwK","type":"genericNode","position":{"x":-173.1401953512206,"y":343.6962336176826},"data":{"type":"URL","node":{"template":{"_type":"Component","code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"import re\n\nfrom langchain_community.document_loaders import AsyncHtmlLoader, WebBaseLoader\n\nfrom langflow.custom import Component\nfrom langflow.helpers.data import data_to_text\nfrom langflow.io import DropdownInput, MessageTextInput, Output\nfrom langflow.schema import Data\nfrom langflow.schema.message import Message\n\n\nclass URLComponent(Component):\n    display_name = \"URL\"\n    description = \"Fetch content from one or more URLs.\"\n    icon = \"layout-template\"\n    name = \"URL\"\n\n    inputs = [\n        MessageTextInput(\n            name=\"urls\",\n            display_name=\"URLs\",\n            info=\"Enter one or more URLs, by clicking the '+' button.\",\n            is_list=True,\n            tool_mode=True,\n        ),\n        DropdownInput(\n            name=\"format\",\n            display_name=\"Output format\",\n            info=\"Output format. Use 'Text' to extract the text from the HTML or 'Raw HTML' for the raw HTML content.\",\n            options=[\"Text\", \"Raw HTML\"],\n            value=\"Text\",\n        ),\n    ]\n\n    outputs = [\n        Output(display_name=\"Data\", name=\"data\", method=\"fetch_content\"),\n        Output(display_name=\"Text\", name=\"text\", method=\"fetch_content_text\"),\n    ]\n\n    def ensure_url(self, string: str) -> str:\n        \"\"\"Ensures the given string is a URL by adding 'http://' if it doesn't start with 'http://' or 'https://'.\n\n        Raises an error if the string is not a valid URL.\n\n        Parameters:\n            string (str): The string to be checked and possibly modified.\n\n        Returns:\n            str: The modified string that is ensured to be a URL.\n\n        Raises:\n            ValueError: If the string is not a valid URL.\n        \"\"\"\n        if not string.startswith((\"http://\", \"https://\")):\n            string = \"http://\" + string\n\n        # Basic URL validation regex\n        url_regex = re.compile(\n            r\"^(https?:\\/\\/)?\"  # optional protocol\n            r\"(www\\.)?\"  # optional www\n            r\"([a-zA-Z0-9.-]+)\"  # domain\n            r\"(\\.[a-zA-Z]{2,})?\"  # top-level domain\n            r\"(:\\d+)?\"  # optional port\n            r\"(\\/[^\\s]*)?$\",  # optional path\n            re.IGNORECASE,\n        )\n\n        if not url_regex.match(string):\n            msg = f\"Invalid URL: {string}\"\n            raise ValueError(msg)\n\n        return string\n\n    def fetch_content(self) -> list[Data]:\n        urls = [self.ensure_url(url.strip()) for url in self.urls if url.strip()]\n        if self.format == \"Raw HTML\":\n            loader = AsyncHtmlLoader(web_path=urls, encoding=\"utf-8\")\n        else:\n            loader = WebBaseLoader(web_paths=urls, encoding=\"utf-8\")\n        docs = loader.load()\n        data = [Data(text=doc.page_content, **doc.metadata) for doc in docs]\n        self.status = data\n        return data\n\n    def fetch_content_text(self) -> Message:\n        data = self.fetch_content()\n\n        result_string = data_to_text(\"{text}\", data)\n        self.status = result_string\n        return Message(text=result_string)\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false},"format":{"tool_mode":false,"trace_as_metadata":true,"options":["Text","Raw HTML"],"combobox":false,"required":false,"placeholder":"","show":true,"name":"format","value":"Text","display_name":"Output format","advanced":false,"dynamic":false,"info":"Output format. Use 'Text' to extract the text from the HTML or 'Raw HTML' for the raw HTML content.","title_case":false,"type":"str","_input_type":"DropdownInput"},"urls":{"tool_mode":true,"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":true,"required":false,"placeholder":"","show":true,"name":"urls","value":["https://jsonlogic.com/","https://jsonlogic.com/operations.html"],"display_name":"URLs","advanced":false,"input_types":["Message"],"dynamic":false,"info":"Enter one or more URLs, by clicking the '+' button.","title_case":false,"type":"str","_input_type":"MessageTextInput"}},"description":"Fetch content from one or more URLs.","icon":"layout-template","base_classes":["Data","Message"],"display_name":"URL","documentation":"","custom_fields":{},"output_types":[],"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Data"],"selected":"Data","name":"data","display_name":"Data","method":"fetch_content","value":"__UNDEFINED__","cache":true},{"types":["Message"],"selected":"Message","name":"text","display_name":"Text","method":"fetch_content_text","value":"__UNDEFINED__","cache":true}],"field_order":["urls","format"],"beta":false,"legacy":false,"edited":false,"metadata":{},"tool_mode":false},"id":"URL-6WzwK","description":"Fetch content from one or more URLs.","display_name":"URL"},"selected":false,"width":320,"height":417,"dragging":false,"positionAbsolute":{"x":-173.1401953512206,"y":343.6962336176826}},{"id":"Prompt-8jxiq","type":"genericNode","position":{"x":2236.087172734187,"y":448.62617646795377},"data":{"type":"Prompt","node":{"template":{"_type":"Component","code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"from langflow.base.prompts.api_utils import process_prompt_template\nfrom langflow.custom import Component\nfrom langflow.inputs.inputs import DefaultPromptField\nfrom langflow.io import Output, PromptInput\nfrom langflow.schema.message import Message\nfrom langflow.template.utils import update_template_values\n\n\nclass PromptComponent(Component):\n    display_name: str = \"Prompt\"\n    description: str = \"Create a prompt template with dynamic variables.\"\n    icon = \"prompts\"\n    trace_type = \"prompt\"\n    name = \"Prompt\"\n\n    inputs = [\n        PromptInput(name=\"template\", display_name=\"Template\"),\n    ]\n\n    outputs = [\n        Output(display_name=\"Prompt Message\", name=\"prompt\", method=\"build_prompt\"),\n    ]\n\n    async def build_prompt(self) -> Message:\n        prompt = Message.from_template(**self._attributes)\n        self.status = prompt.text\n        return prompt\n\n    def _update_template(self, frontend_node: dict):\n        prompt_template = frontend_node[\"template\"][\"template\"][\"value\"]\n        custom_fields = frontend_node[\"custom_fields\"]\n        frontend_node_template = frontend_node[\"template\"]\n        _ = process_prompt_template(\n            template=prompt_template,\n            name=\"template\",\n            custom_fields=custom_fields,\n            frontend_node_template=frontend_node_template,\n        )\n        return frontend_node\n\n    def post_code_processing(self, new_frontend_node: dict, current_frontend_node: dict):\n        \"\"\"This function is called after the code validation is done.\"\"\"\n        frontend_node = super().post_code_processing(new_frontend_node, current_frontend_node)\n        template = frontend_node[\"template\"][\"template\"][\"value\"]\n        # Kept it duplicated for backwards compatibility\n        _ = process_prompt_template(\n            template=template,\n            name=\"template\",\n            custom_fields=frontend_node[\"custom_fields\"],\n            frontend_node_template=frontend_node[\"template\"],\n        )\n        # Now that template is updated, we need to grab any values that were set in the current_frontend_node\n        # and update the frontend_node with those values\n        update_template_values(new_template=frontend_node, previous_template=current_frontend_node[\"template\"])\n        return frontend_node\n\n    def _get_fallback_input(self, **kwargs):\n        return DefaultPromptField(**kwargs)\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false},"template":{"tool_mode":false,"trace_as_input":true,"list":false,"required":false,"placeholder":"","show":true,"name":"template","value":"I am looking to express laws as JsonLogic.\n\nHere is some info on how JSON logic works:\n---\n{jsonLogic}\n---\n\nPlease express the following law as JSON logic:\n{law}\n\nProvide your response as JSON in the following form:\n// The pure JSON logic rule expressed as a JSON object\nrule: object\n// three examples of data that we could run the JsonLogic rule on\nexamples: object[]\n// a list of variables referenced in the rule\nvariables: string[]\n","display_name":"Template","advanced":false,"dynamic":false,"info":"","title_case":false,"type":"prompt","_input_type":"PromptInput","load_from_db":false},"jsonLogic":{"field_type":"str","required":false,"placeholder":"","list":false,"show":true,"multiline":true,"value":"","fileTypes":[],"file_path":"","name":"jsonLogic","display_name":"jsonLogic","advanced":false,"input_types":["Message","Text"],"dynamic":false,"info":"","load_from_db":false,"title_case":false,"type":"str"},"law":{"field_type":"str","required":false,"placeholder":"","list":false,"show":true,"multiline":true,"value":"","fileTypes":[],"file_path":"","name":"law","display_name":"law","advanced":false,"input_types":["Message","Text"],"dynamic":false,"info":"","load_from_db":false,"title_case":false,"type":"str"}},"description":"Create a prompt template with dynamic variables.","icon":"prompts","base_classes":["Message"],"display_name":"Prompt","documentation":"","custom_fields":{"template":["jsonLogic","law"]},"output_types":[],"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Message"],"selected":"Message","name":"prompt","display_name":"Prompt Message","method":"build_prompt","value":"__UNDEFINED__","cache":true}],"field_order":["template"],"beta":false,"legacy":false,"edited":false,"metadata":{},"tool_mode":false,"lf_version":"1.1.0"},"id":"Prompt-8jxiq","description":"Create a prompt template with dynamic variables.","display_name":"Prompt"},"selected":false,"width":320,"height":431,"positionAbsolute":{"x":2236.087172734187,"y":448.62617646795377},"dragging":false},{"id":"OpenAIModel-mexlT","type":"genericNode","position":{"x":636.4832395219315,"y":-152.4587501749921},"data":{"type":"OpenAIModel","node":{"template":{"_type":"Component","output_parser":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"output_parser","value":"","display_name":"Output Parser","advanced":true,"input_types":["OutputParser"],"dynamic":false,"info":"The parser to use to parse the output of the model","title_case":false,"type":"other","_input_type":"HandleInput"},"api_key":{"load_from_db":true,"required":false,"placeholder":"","show":true,"name":"api_key","value":"OPENAI_API_KEY","display_name":"OpenAI API Key","advanced":false,"input_types":["Message"],"dynamic":false,"info":"The OpenAI API Key to use for the OpenAI model.","title_case":false,"password":true,"type":"str","_input_type":"SecretStrInput"},"code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"import operator\nfrom functools import reduce\n\nfrom langchain_openai import ChatOpenAI\nfrom pydantic.v1 import SecretStr\n\nfrom langflow.base.models.model import LCModelComponent\nfrom langflow.base.models.openai_constants import OPENAI_MODEL_NAMES\nfrom langflow.field_typing import LanguageModel\nfrom langflow.field_typing.range_spec import RangeSpec\nfrom langflow.inputs import BoolInput, DictInput, DropdownInput, FloatInput, IntInput, SecretStrInput, StrInput\nfrom langflow.inputs.inputs import HandleInput\n\n\nclass OpenAIModelComponent(LCModelComponent):\n    display_name = \"OpenAI\"\n    description = \"Generates text using OpenAI LLMs.\"\n    icon = \"OpenAI\"\n    name = \"OpenAIModel\"\n\n    inputs = [\n        *LCModelComponent._base_inputs,\n        IntInput(\n            name=\"max_tokens\",\n            display_name=\"Max Tokens\",\n            advanced=True,\n            info=\"The maximum number of tokens to generate. Set to 0 for unlimited tokens.\",\n            range_spec=RangeSpec(min=0, max=128000),\n        ),\n        DictInput(\n            name=\"model_kwargs\",\n            display_name=\"Model Kwargs\",\n            advanced=True,\n            info=\"Additional keyword arguments to pass to the model.\",\n        ),\n        BoolInput(\n            name=\"json_mode\",\n            display_name=\"JSON Mode\",\n            advanced=True,\n            info=\"If True, it will output JSON regardless of passing a schema.\",\n        ),\n        DictInput(\n            name=\"output_schema\",\n            is_list=True,\n            display_name=\"Schema\",\n            advanced=True,\n            info=\"The schema for the Output of the model. \"\n            \"You must pass the word JSON in the prompt. \"\n            \"If left blank, JSON mode will be disabled. [DEPRECATED]\",\n        ),\n        DropdownInput(\n            name=\"model_name\",\n            display_name=\"Model Name\",\n            advanced=False,\n            options=OPENAI_MODEL_NAMES,\n            value=OPENAI_MODEL_NAMES[0],\n        ),\n        StrInput(\n            name=\"openai_api_base\",\n            display_name=\"OpenAI API Base\",\n            advanced=True,\n            info=\"The base URL of the OpenAI API. \"\n            \"Defaults to https://api.openai.com/v1. \"\n            \"You can change this to use other APIs like JinaChat, LocalAI and Prem.\",\n        ),\n        SecretStrInput(\n            name=\"api_key\",\n            display_name=\"OpenAI API Key\",\n            info=\"The OpenAI API Key to use for the OpenAI model.\",\n            advanced=False,\n            value=\"OPENAI_API_KEY\",\n        ),\n        FloatInput(name=\"temperature\", display_name=\"Temperature\", value=0.1),\n        IntInput(\n            name=\"seed\",\n            display_name=\"Seed\",\n            info=\"The seed controls the reproducibility of the job.\",\n            advanced=True,\n            value=1,\n        ),\n        HandleInput(\n            name=\"output_parser\",\n            display_name=\"Output Parser\",\n            info=\"The parser to use to parse the output of the model\",\n            advanced=True,\n            input_types=[\"OutputParser\"],\n        ),\n    ]\n\n    def build_model(self) -> LanguageModel:  # type: ignore[type-var]\n        # self.output_schema is a list of dictionaries\n        # let's convert it to a dictionary\n        output_schema_dict: dict[str, str] = reduce(operator.ior, self.output_schema or {}, {})\n        openai_api_key = self.api_key\n        temperature = self.temperature\n        model_name: str = self.model_name\n        max_tokens = self.max_tokens\n        model_kwargs = self.model_kwargs or {}\n        openai_api_base = self.openai_api_base or \"https://api.openai.com/v1\"\n        json_mode = bool(output_schema_dict) or self.json_mode\n        seed = self.seed\n\n        api_key = SecretStr(openai_api_key).get_secret_value() if openai_api_key else None\n        output = ChatOpenAI(\n            max_tokens=max_tokens or None,\n            model_kwargs=model_kwargs,\n            model=model_name,\n            base_url=openai_api_base,\n            api_key=api_key,\n            temperature=temperature if temperature is not None else 0.1,\n            seed=seed,\n        )\n        if json_mode:\n            if output_schema_dict:\n                output = output.with_structured_output(schema=output_schema_dict, method=\"json_mode\")\n            else:\n                output = output.bind(response_format={\"type\": \"json_object\"})\n\n        return output\n\n    def _get_exception_message(self, e: Exception):\n        \"\"\"Get a message from an OpenAI exception.\n\n        Args:\n            e (Exception): The exception to get the message from.\n\n        Returns:\n            str: The message from the exception.\n        \"\"\"\n        try:\n            from openai import BadRequestError\n        except ImportError:\n            return None\n        if isinstance(e, BadRequestError):\n            message = e.body.get(\"message\")\n            if message:\n                return message\n        return None\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false},"input_value":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"input_value","value":"","display_name":"Input","advanced":false,"input_types":["Message"],"dynamic":false,"info":"","title_case":false,"type":"str","_input_type":"MessageInput"},"json_mode":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"json_mode","value":false,"display_name":"JSON Mode","advanced":true,"dynamic":false,"info":"If True, it will output JSON regardless of passing a schema.","title_case":false,"type":"bool","_input_type":"BoolInput"},"max_tokens":{"trace_as_metadata":true,"range_spec":{"step_type":"float","min":0,"max":128000,"step":0.1},"list":false,"required":false,"placeholder":"","show":true,"name":"max_tokens","value":"","display_name":"Max Tokens","advanced":true,"dynamic":false,"info":"The maximum number of tokens to generate. Set to 0 for unlimited tokens.","title_case":false,"type":"int","_input_type":"IntInput"},"model_kwargs":{"trace_as_input":true,"list":false,"required":false,"placeholder":"","show":true,"name":"model_kwargs","value":{},"display_name":"Model Kwargs","advanced":true,"dynamic":false,"info":"Additional keyword arguments to pass to the model.","title_case":false,"type":"dict","_input_type":"DictInput"},"model_name":{"tool_mode":false,"trace_as_metadata":true,"options":["gpt-4o-mini","gpt-4o","gpt-4-turbo","gpt-4-turbo-preview","gpt-4","gpt-3.5-turbo","gpt-3.5-turbo-0125"],"combobox":false,"required":false,"placeholder":"","show":true,"name":"model_name","value":"gpt-4o","display_name":"Model Name","advanced":false,"dynamic":false,"info":"","title_case":false,"type":"str","_input_type":"DropdownInput","load_from_db":false},"openai_api_base":{"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"openai_api_base","value":"","display_name":"OpenAI API Base","advanced":true,"dynamic":false,"info":"The base URL of the OpenAI API. Defaults to https://api.openai.com/v1. You can change this to use other APIs like JinaChat, LocalAI and Prem.","title_case":false,"type":"str","_input_type":"StrInput"},"output_schema":{"trace_as_input":true,"list":true,"required":false,"placeholder":"","show":true,"name":"output_schema","value":{},"display_name":"Schema","advanced":true,"dynamic":false,"info":"The schema for the Output of the model. You must pass the word JSON in the prompt. If left blank, JSON mode will be disabled. [DEPRECATED]","title_case":false,"type":"dict","_input_type":"DictInput"},"seed":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"seed","value":1,"display_name":"Seed","advanced":true,"dynamic":false,"info":"The seed controls the reproducibility of the job.","title_case":false,"type":"int","_input_type":"IntInput"},"stream":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"stream","value":false,"display_name":"Stream","advanced":false,"dynamic":false,"info":"Stream the response from the model. Streaming works only in Chat.","title_case":false,"type":"bool","_input_type":"BoolInput"},"system_message":{"tool_mode":false,"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"system_message","value":"","display_name":"System Message","advanced":false,"input_types":["Message"],"dynamic":false,"info":"System message to pass to the model.","title_case":false,"type":"str","_input_type":"MessageTextInput"},"temperature":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"temperature","value":0,"display_name":"Temperature","advanced":false,"dynamic":false,"info":"","title_case":false,"type":"float","_input_type":"FloatInput","load_from_db":false}},"description":"Generates text using OpenAI LLMs.","icon":"OpenAI","base_classes":["LanguageModel","Message"],"display_name":"OpenAI","documentation":"","custom_fields":{},"output_types":[],"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Message"],"selected":"Message","name":"text_output","display_name":"Text","method":"text_response","value":"__UNDEFINED__","cache":true,"required_inputs":[]},{"types":["LanguageModel"],"selected":"LanguageModel","name":"model_output","display_name":"Language Model","method":"build_model","value":"__UNDEFINED__","cache":true,"required_inputs":[]}],"field_order":["input_value","system_message","stream","max_tokens","model_kwargs","json_mode","output_schema","model_name","openai_api_base","api_key","temperature","seed","output_parser"],"beta":false,"legacy":false,"edited":false,"metadata":{},"tool_mode":false},"id":"OpenAIModel-mexlT","description":"Generates text using OpenAI LLMs.","display_name":"OpenAI"},"selected":false,"width":320,"height":671,"dragging":false,"positionAbsolute":{"x":636.4832395219315,"y":-152.4587501749921}},{"id":"Prompt-7Pxkb","type":"genericNode","position":{"x":173.24459817040554,"y":-69.41596997590636},"data":{"type":"Prompt","node":{"template":{"_type":"Component","code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"from langflow.base.prompts.api_utils import process_prompt_template\nfrom langflow.custom import Component\nfrom langflow.inputs.inputs import DefaultPromptField\nfrom langflow.io import Output, PromptInput\nfrom langflow.schema.message import Message\nfrom langflow.template.utils import update_template_values\n\n\nclass PromptComponent(Component):\n    display_name: str = \"Prompt\"\n    description: str = \"Create a prompt template with dynamic variables.\"\n    icon = \"prompts\"\n    trace_type = \"prompt\"\n    name = \"Prompt\"\n\n    inputs = [\n        PromptInput(name=\"template\", display_name=\"Template\"),\n    ]\n\n    outputs = [\n        Output(display_name=\"Prompt Message\", name=\"prompt\", method=\"build_prompt\"),\n    ]\n\n    async def build_prompt(self) -> Message:\n        prompt = Message.from_template(**self._attributes)\n        self.status = prompt.text\n        return prompt\n\n    def _update_template(self, frontend_node: dict):\n        prompt_template = frontend_node[\"template\"][\"template\"][\"value\"]\n        custom_fields = frontend_node[\"custom_fields\"]\n        frontend_node_template = frontend_node[\"template\"]\n        _ = process_prompt_template(\n            template=prompt_template,\n            name=\"template\",\n            custom_fields=custom_fields,\n            frontend_node_template=frontend_node_template,\n        )\n        return frontend_node\n\n    def post_code_processing(self, new_frontend_node: dict, current_frontend_node: dict):\n        \"\"\"This function is called after the code validation is done.\"\"\"\n        frontend_node = super().post_code_processing(new_frontend_node, current_frontend_node)\n        template = frontend_node[\"template\"][\"template\"][\"value\"]\n        # Kept it duplicated for backwards compatibility\n        _ = process_prompt_template(\n            template=template,\n            name=\"template\",\n            custom_fields=frontend_node[\"custom_fields\"],\n            frontend_node_template=frontend_node[\"template\"],\n        )\n        # Now that template is updated, we need to grab any values that were set in the current_frontend_node\n        # and update the frontend_node with those values\n        update_template_values(new_template=frontend_node, previous_template=current_frontend_node[\"template\"])\n        return frontend_node\n\n    def _get_fallback_input(self, **kwargs):\n        return DefaultPromptField(**kwargs)\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false},"template":{"tool_mode":false,"trace_as_input":true,"list":false,"required":false,"placeholder":"","show":true,"name":"template","value":"You are a principal software engineer and your job is to create a complete DSL specification for JsonLogic. \nYou are given the following documentation on JsonLogic and your task is to extract all supported operations and provide an example for each.\nIgnore information that does not pertain to JsonLogic. \nDo NOT document any information that is not outlined in the following documentation:\n---\n{jsonLogic}\n\n---","display_name":"Template","advanced":false,"dynamic":false,"info":"","title_case":false,"type":"prompt","_input_type":"PromptInput","load_from_db":false},"jsonLogic":{"field_type":"str","required":false,"placeholder":"","list":false,"show":true,"multiline":true,"value":"","fileTypes":[],"file_path":"","name":"jsonLogic","display_name":"jsonLogic","advanced":false,"input_types":["Message","Text"],"dynamic":false,"info":"","load_from_db":false,"title_case":false,"type":"str"}},"description":"Create a prompt template with dynamic variables.","icon":"prompts","base_classes":["Message"],"display_name":"Prompt","documentation":"","custom_fields":{"template":["jsonLogic"]},"output_types":[],"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Message"],"selected":"Message","name":"prompt","display_name":"Prompt Message","method":"build_prompt","value":"__UNDEFINED__","cache":true}],"field_order":["template"],"beta":false,"legacy":false,"edited":false,"metadata":{},"tool_mode":false},"id":"Prompt-7Pxkb","description":"Create a prompt template with dynamic variables.","display_name":"Prompt"},"selected":false,"width":320,"height":345,"positionAbsolute":{"x":173.24459817040554,"y":-69.41596997590636},"dragging":false},{"id":"OpenAIModel-Aofzl","type":"genericNode","position":{"x":476.55439645974593,"y":1303.2510456464277},"data":{"type":"OpenAIModel","node":{"template":{"_type":"Component","output_parser":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"output_parser","value":"","display_name":"Output Parser","advanced":true,"input_types":["OutputParser"],"dynamic":false,"info":"The parser to use to parse the output of the model","title_case":false,"type":"other","_input_type":"HandleInput"},"api_key":{"load_from_db":true,"required":false,"placeholder":"","show":true,"name":"api_key","value":"OPENAI_API_KEY","display_name":"OpenAI API Key","advanced":false,"input_types":["Message"],"dynamic":false,"info":"The OpenAI API Key to use for the OpenAI model.","title_case":false,"password":true,"type":"str","_input_type":"SecretStrInput"},"code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"import operator\nfrom functools import reduce\n\nfrom langchain_openai import ChatOpenAI\nfrom pydantic.v1 import SecretStr\n\nfrom langflow.base.models.model import LCModelComponent\nfrom langflow.base.models.openai_constants import OPENAI_MODEL_NAMES\nfrom langflow.field_typing import LanguageModel\nfrom langflow.field_typing.range_spec import RangeSpec\nfrom langflow.inputs import BoolInput, DictInput, DropdownInput, FloatInput, IntInput, SecretStrInput, StrInput\nfrom langflow.inputs.inputs import HandleInput\n\n\nclass OpenAIModelComponent(LCModelComponent):\n    display_name = \"OpenAI\"\n    description = \"Generates text using OpenAI LLMs.\"\n    icon = \"OpenAI\"\n    name = \"OpenAIModel\"\n\n    inputs = [\n        *LCModelComponent._base_inputs,\n        IntInput(\n            name=\"max_tokens\",\n            display_name=\"Max Tokens\",\n            advanced=True,\n            info=\"The maximum number of tokens to generate. Set to 0 for unlimited tokens.\",\n            range_spec=RangeSpec(min=0, max=128000),\n        ),\n        DictInput(\n            name=\"model_kwargs\",\n            display_name=\"Model Kwargs\",\n            advanced=True,\n            info=\"Additional keyword arguments to pass to the model.\",\n        ),\n        BoolInput(\n            name=\"json_mode\",\n            display_name=\"JSON Mode\",\n            advanced=True,\n            info=\"If True, it will output JSON regardless of passing a schema.\",\n        ),\n        DictInput(\n            name=\"output_schema\",\n            is_list=True,\n            display_name=\"Schema\",\n            advanced=True,\n            info=\"The schema for the Output of the model. \"\n            \"You must pass the word JSON in the prompt. \"\n            \"If left blank, JSON mode will be disabled. [DEPRECATED]\",\n        ),\n        DropdownInput(\n            name=\"model_name\",\n            display_name=\"Model Name\",\n            advanced=False,\n            options=OPENAI_MODEL_NAMES,\n            value=OPENAI_MODEL_NAMES[0],\n        ),\n        StrInput(\n            name=\"openai_api_base\",\n            display_name=\"OpenAI API Base\",\n            advanced=True,\n            info=\"The base URL of the OpenAI API. \"\n            \"Defaults to https://api.openai.com/v1. \"\n            \"You can change this to use other APIs like JinaChat, LocalAI and Prem.\",\n        ),\n        SecretStrInput(\n            name=\"api_key\",\n            display_name=\"OpenAI API Key\",\n            info=\"The OpenAI API Key to use for the OpenAI model.\",\n            advanced=False,\n            value=\"OPENAI_API_KEY\",\n        ),\n        FloatInput(name=\"temperature\", display_name=\"Temperature\", value=0.1),\n        IntInput(\n            name=\"seed\",\n            display_name=\"Seed\",\n            info=\"The seed controls the reproducibility of the job.\",\n            advanced=True,\n            value=1,\n        ),\n        HandleInput(\n            name=\"output_parser\",\n            display_name=\"Output Parser\",\n            info=\"The parser to use to parse the output of the model\",\n            advanced=True,\n            input_types=[\"OutputParser\"],\n        ),\n    ]\n\n    def build_model(self) -> LanguageModel:  # type: ignore[type-var]\n        # self.output_schema is a list of dictionaries\n        # let's convert it to a dictionary\n        output_schema_dict: dict[str, str] = reduce(operator.ior, self.output_schema or {}, {})\n        openai_api_key = self.api_key\n        temperature = self.temperature\n        model_name: str = self.model_name\n        max_tokens = self.max_tokens\n        model_kwargs = self.model_kwargs or {}\n        openai_api_base = self.openai_api_base or \"https://api.openai.com/v1\"\n        json_mode = bool(output_schema_dict) or self.json_mode\n        seed = self.seed\n\n        api_key = SecretStr(openai_api_key).get_secret_value() if openai_api_key else None\n        output = ChatOpenAI(\n            max_tokens=max_tokens or None,\n            model_kwargs=model_kwargs,\n            model=model_name,\n            base_url=openai_api_base,\n            api_key=api_key,\n            temperature=temperature if temperature is not None else 0.1,\n            seed=seed,\n        )\n        if json_mode:\n            if output_schema_dict:\n                output = output.with_structured_output(schema=output_schema_dict, method=\"json_mode\")\n            else:\n                output = output.bind(response_format={\"type\": \"json_object\"})\n\n        return output\n\n    def _get_exception_message(self, e: Exception):\n        \"\"\"Get a message from an OpenAI exception.\n\n        Args:\n            e (Exception): The exception to get the message from.\n\n        Returns:\n            str: The message from the exception.\n        \"\"\"\n        try:\n            from openai import BadRequestError\n        except ImportError:\n            return None\n        if isinstance(e, BadRequestError):\n            message = e.body.get(\"message\")\n            if message:\n                return message\n        return None\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false},"input_value":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"input_value","value":"","display_name":"Input","advanced":false,"input_types":["Message"],"dynamic":false,"info":"","title_case":false,"type":"str","_input_type":"MessageInput"},"json_mode":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"json_mode","value":false,"display_name":"JSON Mode","advanced":true,"dynamic":false,"info":"If True, it will output JSON regardless of passing a schema.","title_case":false,"type":"bool","_input_type":"BoolInput"},"max_tokens":{"trace_as_metadata":true,"range_spec":{"step_type":"float","min":0,"max":128000,"step":0.1},"list":false,"required":false,"placeholder":"","show":true,"name":"max_tokens","value":"","display_name":"Max Tokens","advanced":true,"dynamic":false,"info":"The maximum number of tokens to generate. Set to 0 for unlimited tokens.","title_case":false,"type":"int","_input_type":"IntInput"},"model_kwargs":{"trace_as_input":true,"list":false,"required":false,"placeholder":"","show":true,"name":"model_kwargs","value":{},"display_name":"Model Kwargs","advanced":true,"dynamic":false,"info":"Additional keyword arguments to pass to the model.","title_case":false,"type":"dict","_input_type":"DictInput"},"model_name":{"tool_mode":false,"trace_as_metadata":true,"options":["gpt-4o-mini","gpt-4o","gpt-4-turbo","gpt-4-turbo-preview","gpt-4","gpt-3.5-turbo","gpt-3.5-turbo-0125"],"combobox":false,"required":false,"placeholder":"","show":true,"name":"model_name","value":"gpt-4o","display_name":"Model Name","advanced":false,"dynamic":false,"info":"","title_case":false,"type":"str","_input_type":"DropdownInput","load_from_db":false},"openai_api_base":{"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"openai_api_base","value":"","display_name":"OpenAI API Base","advanced":true,"dynamic":false,"info":"The base URL of the OpenAI API. Defaults to https://api.openai.com/v1. You can change this to use other APIs like JinaChat, LocalAI and Prem.","title_case":false,"type":"str","_input_type":"StrInput"},"output_schema":{"trace_as_input":true,"list":true,"required":false,"placeholder":"","show":true,"name":"output_schema","value":{},"display_name":"Schema","advanced":true,"dynamic":false,"info":"The schema for the Output of the model. You must pass the word JSON in the prompt. If left blank, JSON mode will be disabled. [DEPRECATED]","title_case":false,"type":"dict","_input_type":"DictInput"},"seed":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"seed","value":1,"display_name":"Seed","advanced":true,"dynamic":false,"info":"The seed controls the reproducibility of the job.","title_case":false,"type":"int","_input_type":"IntInput"},"stream":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"stream","value":false,"display_name":"Stream","advanced":false,"dynamic":false,"info":"Stream the response from the model. Streaming works only in Chat.","title_case":false,"type":"bool","_input_type":"BoolInput"},"system_message":{"tool_mode":false,"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"system_message","value":"","display_name":"System Message","advanced":false,"input_types":["Message"],"dynamic":false,"info":"System message to pass to the model.","title_case":false,"type":"str","_input_type":"MessageTextInput"},"temperature":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"temperature","value":0,"display_name":"Temperature","advanced":false,"dynamic":false,"info":"","title_case":false,"type":"float","_input_type":"FloatInput","load_from_db":false}},"description":"Generates text using OpenAI LLMs.","icon":"OpenAI","base_classes":["LanguageModel","Message"],"display_name":"OpenAI","documentation":"","custom_fields":{},"output_types":[],"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Message"],"selected":"Message","name":"text_output","display_name":"Text","method":"text_response","value":"__UNDEFINED__","cache":true,"required_inputs":[]},{"types":["LanguageModel"],"selected":"LanguageModel","name":"model_output","display_name":"Language Model","method":"build_model","value":"__UNDEFINED__","cache":true,"required_inputs":[]}],"field_order":["input_value","system_message","stream","max_tokens","model_kwargs","json_mode","output_schema","model_name","openai_api_base","api_key","temperature","seed","output_parser"],"beta":false,"legacy":false,"edited":false,"metadata":{},"tool_mode":false},"id":"OpenAIModel-Aofzl","description":"Generates text using OpenAI LLMs.","display_name":"OpenAI"},"selected":false,"width":320,"height":671,"positionAbsolute":{"x":476.55439645974593,"y":1303.2510456464277},"dragging":false},{"id":"Prompt-aIaC7","type":"genericNode","position":{"x":476.92216824642264,"y":769.6220753460187},"data":{"type":"Prompt","node":{"template":{"_type":"Component","code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"from langflow.base.prompts.api_utils import process_prompt_template\nfrom langflow.custom import Component\nfrom langflow.inputs.inputs import DefaultPromptField\nfrom langflow.io import Output, PromptInput\nfrom langflow.schema.message import Message\nfrom langflow.template.utils import update_template_values\n\n\nclass PromptComponent(Component):\n    display_name: str = \"Prompt\"\n    description: str = \"Create a prompt template with dynamic variables.\"\n    icon = \"prompts\"\n    trace_type = \"prompt\"\n    name = \"Prompt\"\n\n    inputs = [\n        PromptInput(name=\"template\", display_name=\"Template\"),\n    ]\n\n    outputs = [\n        Output(display_name=\"Prompt Message\", name=\"prompt\", method=\"build_prompt\"),\n    ]\n\n    async def build_prompt(self) -> Message:\n        prompt = Message.from_template(**self._attributes)\n        self.status = prompt.text\n        return prompt\n\n    def _update_template(self, frontend_node: dict):\n        prompt_template = frontend_node[\"template\"][\"template\"][\"value\"]\n        custom_fields = frontend_node[\"custom_fields\"]\n        frontend_node_template = frontend_node[\"template\"]\n        _ = process_prompt_template(\n            template=prompt_template,\n            name=\"template\",\n            custom_fields=custom_fields,\n            frontend_node_template=frontend_node_template,\n        )\n        return frontend_node\n\n    def post_code_processing(self, new_frontend_node: dict, current_frontend_node: dict):\n        \"\"\"This function is called after the code validation is done.\"\"\"\n        frontend_node = super().post_code_processing(new_frontend_node, current_frontend_node)\n        template = frontend_node[\"template\"][\"template\"][\"value\"]\n        # Kept it duplicated for backwards compatibility\n        _ = process_prompt_template(\n            template=template,\n            name=\"template\",\n            custom_fields=frontend_node[\"custom_fields\"],\n            frontend_node_template=frontend_node[\"template\"],\n        )\n        # Now that template is updated, we need to grab any values that were set in the current_frontend_node\n        # and update the frontend_node with those values\n        update_template_values(new_template=frontend_node, previous_template=current_frontend_node[\"template\"])\n        return frontend_node\n\n    def _get_fallback_input(self, **kwargs):\n        return DefaultPromptField(**kwargs)\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false},"template":{"tool_mode":false,"trace_as_input":true,"list":false,"required":false,"placeholder":"","show":true,"name":"template","value":"You are a principal software engineer reviewing the work of another engineer. You will provide an honest review of their work, outlining mistakes they made, if any. If they made no mistakes, respond \"LGTM\".\n\nTheir job was to create a complete DSL specification for JsonLogic. \nThey were given the following documentation on JsonLogic and asked to extract all supported operations and provide an example for each.\nThey were also asked to ignore information that does not pertain to JsonLogic and NOT document any information that is not outlined in the following documentation:\nOriginal Documentation:\n---\n{jsonLogic}\n\n---\n\nTheir DSL specification\n{jsonLogicSpec}","display_name":"Template","advanced":false,"dynamic":false,"info":"","title_case":false,"type":"prompt","_input_type":"PromptInput","load_from_db":false},"jsonLogic":{"field_type":"str","required":false,"placeholder":"","list":false,"show":true,"multiline":true,"value":"","fileTypes":[],"file_path":"","name":"jsonLogic","display_name":"jsonLogic","advanced":false,"input_types":["Message","Text"],"dynamic":false,"info":"","load_from_db":false,"title_case":false,"type":"str"},"jsonLogicSpec":{"field_type":"str","required":false,"placeholder":"","list":false,"show":true,"multiline":true,"value":"","fileTypes":[],"file_path":"","name":"jsonLogicSpec","display_name":"jsonLogicSpec","advanced":false,"input_types":["Message","Text"],"dynamic":false,"info":"","load_from_db":false,"title_case":false,"type":"str"}},"description":"Create a prompt template with dynamic variables.","icon":"prompts","base_classes":["Message"],"display_name":"Reviewer Prompt ","documentation":"","custom_fields":{"template":["jsonLogic","jsonLogicSpec"]},"output_types":[],"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Message"],"selected":"Message","name":"prompt","display_name":"Prompt Message","method":"build_prompt","value":"__UNDEFINED__","cache":true}],"field_order":["template"],"beta":false,"legacy":false,"edited":false,"metadata":{},"tool_mode":false},"id":"Prompt-aIaC7","description":"Create a prompt template with dynamic variables.","display_name":"Reviewer Prompt "},"selected":false,"width":320,"height":431,"positionAbsolute":{"x":476.92216824642264,"y":769.6220753460187},"dragging":false},{"id":"TextInput-i0IHc","type":"genericNode","position":{"x":1710.3157261822696,"y":861.4095873982451},"data":{"type":"TextInput","node":{"template":{"_type":"Component","code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"from langflow.base.io.text import TextComponent\nfrom langflow.io import MultilineInput, Output\nfrom langflow.schema.message import Message\n\n\nclass TextInputComponent(TextComponent):\n    display_name = \"Text Input\"\n    description = \"Get text inputs from the Playground.\"\n    icon = \"type\"\n    name = \"TextInput\"\n\n    inputs = [\n        MultilineInput(\n            name=\"input_value\",\n            display_name=\"Text\",\n            info=\"Text to be passed as input.\",\n        ),\n    ]\n    outputs = [\n        Output(display_name=\"Text\", name=\"text\", method=\"text_response\"),\n    ]\n\n    def text_response(self) -> Message:\n        return Message(\n            text=self.input_value,\n        )\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false},"input_value":{"trace_as_input":true,"multiline":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"input_value","value":"The Honeycrisp apple is the official fruit of the state of Minnesota.","display_name":"Text","advanced":false,"input_types":["Message"],"dynamic":false,"info":"Text to be passed as input.","title_case":false,"type":"str","_input_type":"MultilineInput"}},"description":"Get text inputs from the Playground.","icon":"type","base_classes":["Message"],"display_name":"Text Input","documentation":"","custom_fields":{},"output_types":[],"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Message"],"selected":"Message","name":"text","display_name":"Text","method":"text_response","value":"__UNDEFINED__","cache":true}],"field_order":["input_value"],"beta":false,"edited":false,"metadata":{},"lf_version":"1.1.0"},"id":"TextInput-i0IHc"},"selected":false,"width":320,"height":233,"positionAbsolute":{"x":1710.3157261822696,"y":861.4095873982451},"dragging":false},{"id":"OpenAIModel-CQetX","type":"genericNode","position":{"x":2787.7999815776725,"y":473.36000309438873},"data":{"type":"OpenAIModel","node":{"template":{"_type":"Component","output_parser":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"output_parser","value":"","display_name":"Output Parser","advanced":true,"input_types":["OutputParser"],"dynamic":false,"info":"The parser to use to parse the output of the model","title_case":false,"type":"other","_input_type":"HandleInput"},"api_key":{"load_from_db":true,"required":false,"placeholder":"","show":true,"name":"api_key","value":"OPENAI_API_KEY","display_name":"OpenAI API Key","advanced":false,"input_types":["Message"],"dynamic":false,"info":"The OpenAI API Key to use for the OpenAI model.","title_case":false,"password":true,"type":"str","_input_type":"SecretStrInput"},"code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"import operator\nfrom functools import reduce\n\nfrom langchain_openai import ChatOpenAI\nfrom pydantic.v1 import SecretStr\n\nfrom langflow.base.models.model import LCModelComponent\nfrom langflow.base.models.openai_constants import OPENAI_MODEL_NAMES\nfrom langflow.field_typing import LanguageModel\nfrom langflow.field_typing.range_spec import RangeSpec\nfrom langflow.inputs import BoolInput, DictInput, DropdownInput, FloatInput, IntInput, SecretStrInput, StrInput\nfrom langflow.inputs.inputs import HandleInput\n\n\nclass OpenAIModelComponent(LCModelComponent):\n    display_name = \"OpenAI\"\n    description = \"Generates text using OpenAI LLMs.\"\n    icon = \"OpenAI\"\n    name = \"OpenAIModel\"\n\n    inputs = [\n        *LCModelComponent._base_inputs,\n        IntInput(\n            name=\"max_tokens\",\n            display_name=\"Max Tokens\",\n            advanced=True,\n            info=\"The maximum number of tokens to generate. Set to 0 for unlimited tokens.\",\n            range_spec=RangeSpec(min=0, max=128000),\n        ),\n        DictInput(\n            name=\"model_kwargs\",\n            display_name=\"Model Kwargs\",\n            advanced=True,\n            info=\"Additional keyword arguments to pass to the model.\",\n        ),\n        BoolInput(\n            name=\"json_mode\",\n            display_name=\"JSON Mode\",\n            advanced=True,\n            info=\"If True, it will output JSON regardless of passing a schema.\",\n        ),\n        DictInput(\n            name=\"output_schema\",\n            is_list=True,\n            display_name=\"Schema\",\n            advanced=True,\n            info=\"The schema for the Output of the model. \"\n            \"You must pass the word JSON in the prompt. \"\n            \"If left blank, JSON mode will be disabled. [DEPRECATED]\",\n        ),\n        DropdownInput(\n            name=\"model_name\",\n            display_name=\"Model Name\",\n            advanced=False,\n            options=OPENAI_MODEL_NAMES,\n            value=OPENAI_MODEL_NAMES[0],\n        ),\n        StrInput(\n            name=\"openai_api_base\",\n            display_name=\"OpenAI API Base\",\n            advanced=True,\n            info=\"The base URL of the OpenAI API. \"\n            \"Defaults to https://api.openai.com/v1. \"\n            \"You can change this to use other APIs like JinaChat, LocalAI and Prem.\",\n        ),\n        SecretStrInput(\n            name=\"api_key\",\n            display_name=\"OpenAI API Key\",\n            info=\"The OpenAI API Key to use for the OpenAI model.\",\n            advanced=False,\n            value=\"OPENAI_API_KEY\",\n        ),\n        FloatInput(name=\"temperature\", display_name=\"Temperature\", value=0.1),\n        IntInput(\n            name=\"seed\",\n            display_name=\"Seed\",\n            info=\"The seed controls the reproducibility of the job.\",\n            advanced=True,\n            value=1,\n        ),\n        HandleInput(\n            name=\"output_parser\",\n            display_name=\"Output Parser\",\n            info=\"The parser to use to parse the output of the model\",\n            advanced=True,\n            input_types=[\"OutputParser\"],\n        ),\n    ]\n\n    def build_model(self) -> LanguageModel:  # type: ignore[type-var]\n        # self.output_schema is a list of dictionaries\n        # let's convert it to a dictionary\n        output_schema_dict: dict[str, str] = reduce(operator.ior, self.output_schema or {}, {})\n        openai_api_key = self.api_key\n        temperature = self.temperature\n        model_name: str = self.model_name\n        max_tokens = self.max_tokens\n        model_kwargs = self.model_kwargs or {}\n        openai_api_base = self.openai_api_base or \"https://api.openai.com/v1\"\n        json_mode = bool(output_schema_dict) or self.json_mode\n        seed = self.seed\n\n        api_key = SecretStr(openai_api_key).get_secret_value() if openai_api_key else None\n        output = ChatOpenAI(\n            max_tokens=max_tokens or None,\n            model_kwargs=model_kwargs,\n            model=model_name,\n            base_url=openai_api_base,\n            api_key=api_key,\n            temperature=temperature if temperature is not None else 0.1,\n            seed=seed,\n        )\n        if json_mode:\n            if output_schema_dict:\n                output = output.with_structured_output(schema=output_schema_dict, method=\"json_mode\")\n            else:\n                output = output.bind(response_format={\"type\": \"json_object\"})\n\n        return output\n\n    def _get_exception_message(self, e: Exception):\n        \"\"\"Get a message from an OpenAI exception.\n\n        Args:\n            e (Exception): The exception to get the message from.\n\n        Returns:\n            str: The message from the exception.\n        \"\"\"\n        try:\n            from openai import BadRequestError\n        except ImportError:\n            return None\n        if isinstance(e, BadRequestError):\n            message = e.body.get(\"message\")\n            if message:\n                return message\n        return None\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false},"input_value":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"input_value","value":"","display_name":"Input","advanced":false,"input_types":["Message"],"dynamic":false,"info":"","title_case":false,"type":"str","_input_type":"MessageInput"},"json_mode":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"json_mode","value":false,"display_name":"JSON Mode","advanced":true,"dynamic":false,"info":"If True, it will output JSON regardless of passing a schema.","title_case":false,"type":"bool","_input_type":"BoolInput"},"max_tokens":{"trace_as_metadata":true,"range_spec":{"step_type":"float","min":0,"max":128000,"step":0.1},"list":false,"required":false,"placeholder":"","show":true,"name":"max_tokens","value":"","display_name":"Max Tokens","advanced":true,"dynamic":false,"info":"The maximum number of tokens to generate. Set to 0 for unlimited tokens.","title_case":false,"type":"int","_input_type":"IntInput"},"model_kwargs":{"trace_as_input":true,"list":false,"required":false,"placeholder":"","show":true,"name":"model_kwargs","value":{},"display_name":"Model Kwargs","advanced":true,"dynamic":false,"info":"Additional keyword arguments to pass to the model.","title_case":false,"type":"dict","_input_type":"DictInput"},"model_name":{"tool_mode":false,"trace_as_metadata":true,"options":["gpt-4o-mini","gpt-4o","gpt-4-turbo","gpt-4-turbo-preview","gpt-4","gpt-3.5-turbo","gpt-3.5-turbo-0125"],"combobox":false,"required":false,"placeholder":"","show":true,"name":"model_name","value":"gpt-4o","display_name":"Model Name","advanced":false,"dynamic":false,"info":"","title_case":false,"type":"str","_input_type":"DropdownInput","load_from_db":false},"openai_api_base":{"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"openai_api_base","value":"","display_name":"OpenAI API Base","advanced":true,"dynamic":false,"info":"The base URL of the OpenAI API. Defaults to https://api.openai.com/v1. You can change this to use other APIs like JinaChat, LocalAI and Prem.","title_case":false,"type":"str","_input_type":"StrInput"},"output_schema":{"trace_as_input":true,"list":true,"required":false,"placeholder":"","show":true,"name":"output_schema","value":{},"display_name":"Schema","advanced":true,"dynamic":false,"info":"The schema for the Output of the model. You must pass the word JSON in the prompt. If left blank, JSON mode will be disabled. [DEPRECATED]","title_case":false,"type":"dict","_input_type":"DictInput"},"seed":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"seed","value":1,"display_name":"Seed","advanced":true,"dynamic":false,"info":"The seed controls the reproducibility of the job.","title_case":false,"type":"int","_input_type":"IntInput"},"stream":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"stream","value":false,"display_name":"Stream","advanced":false,"dynamic":false,"info":"Stream the response from the model. Streaming works only in Chat.","title_case":false,"type":"bool","_input_type":"BoolInput"},"system_message":{"tool_mode":false,"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"system_message","value":"","display_name":"System Message","advanced":false,"input_types":["Message"],"dynamic":false,"info":"System message to pass to the model.","title_case":false,"type":"str","_input_type":"MessageTextInput"},"temperature":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"temperature","value":0,"display_name":"Temperature","advanced":false,"dynamic":false,"info":"","title_case":false,"type":"float","_input_type":"FloatInput","load_from_db":false}},"description":"Generates text using OpenAI LLMs.","icon":"OpenAI","base_classes":["LanguageModel","Message"],"display_name":"OpenAI","documentation":"","custom_fields":{},"output_types":[],"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Message"],"selected":"Message","name":"text_output","display_name":"Text","method":"text_response","value":"__UNDEFINED__","cache":true,"required_inputs":[]},{"types":["LanguageModel"],"selected":"LanguageModel","name":"model_output","display_name":"Language Model","method":"build_model","value":"__UNDEFINED__","cache":true,"required_inputs":[]}],"field_order":["input_value","system_message","stream","max_tokens","model_kwargs","json_mode","output_schema","model_name","openai_api_base","api_key","temperature","seed","output_parser"],"beta":false,"legacy":false,"edited":false,"metadata":{},"tool_mode":false},"id":"OpenAIModel-CQetX","description":"Generates text using OpenAI LLMs.","display_name":"OpenAI"},"selected":false,"width":320,"height":671,"positionAbsolute":{"x":2787.7999815776725,"y":473.36000309438873},"dragging":false},{"id":"JSONtoData-Ta1dv","type":"genericNode","position":{"x":3329.4763311498004,"y":536.0354101674794},"data":{"type":"JSONtoData","node":{"template":{"_type":"Component","json_file":{"trace_as_metadata":true,"file_path":"","fileTypes":["json"],"list":false,"required":false,"placeholder":"","show":true,"name":"json_file","value":"","display_name":"JSON File","advanced":false,"dynamic":false,"info":"Upload a JSON file to convert to a Data object or list of Data objects","title_case":false,"type":"file","_input_type":"FileInput"},"code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"import json\nfrom pathlib import Path\n\nfrom json_repair import repair_json\n\nfrom langflow.custom import Component\nfrom langflow.io import FileInput, MessageTextInput, MultilineInput, Output\nfrom langflow.schema import Data\n\n\nclass JSONToDataComponent(Component):\n    display_name = \"Load JSON\"\n    description = (\n        \"Convert a JSON file, JSON from a file path, or a JSON string to a Data object or a list of Data objects\"\n    )\n    icon = \"braces\"\n    name = \"JSONtoData\"\n    legacy = True\n\n    inputs = [\n        FileInput(\n            name=\"json_file\",\n            display_name=\"JSON File\",\n            file_types=[\"json\"],\n            info=\"Upload a JSON file to convert to a Data object or list of Data objects\",\n        ),\n        MessageTextInput(\n            name=\"json_path\",\n            display_name=\"JSON File Path\",\n            info=\"Provide the path to the JSON file as pure text\",\n        ),\n        MultilineInput(\n            name=\"json_string\",\n            display_name=\"JSON String\",\n            info=\"Enter a valid JSON string (object or array) to convert to a Data object or list of Data objects\",\n        ),\n    ]\n\n    outputs = [\n        Output(name=\"data\", display_name=\"Data\", method=\"convert_json_to_data\"),\n    ]\n\n    def convert_json_to_data(self) -> Data | list[Data]:\n        if sum(bool(field) for field in [self.json_file, self.json_path, self.json_string]) != 1:\n            msg = \"Please provide exactly one of: JSON file, file path, or JSON string.\"\n            self.status = msg\n            raise ValueError(msg)\n\n        json_data = None\n\n        try:\n            if self.json_file:\n                resolved_path = self.resolve_path(self.json_file)\n                file_path = Path(resolved_path)\n                if file_path.suffix.lower() != \".json\":\n                    self.status = \"The provided file must be a JSON file.\"\n                else:\n                    json_data = file_path.read_text(encoding=\"utf-8\")\n\n            elif self.json_path:\n                file_path = Path(self.json_path)\n                if file_path.suffix.lower() != \".json\":\n                    self.status = \"The provided file must be a JSON file.\"\n                else:\n                    json_data = file_path.read_text(encoding=\"utf-8\")\n\n            else:\n                json_data = self.json_string\n\n            if json_data:\n                # Try to parse the JSON string\n                try:\n                    parsed_data = json.loads(json_data)\n                except json.JSONDecodeError:\n                    # If JSON parsing fails, try to repair the JSON string\n                    repaired_json_string = repair_json(json_data)\n                    parsed_data = json.loads(repaired_json_string)\n\n                # Check if the parsed data is a list\n                if isinstance(parsed_data, list):\n                    result = [Data(data=item) for item in parsed_data]\n                else:\n                    result = Data(data=parsed_data)\n                self.status = result\n                return result\n\n        except (json.JSONDecodeError, SyntaxError, ValueError) as e:\n            error_message = f\"Invalid JSON or Python literal: {e}\"\n            self.status = error_message\n            raise ValueError(error_message) from e\n\n        except Exception as e:\n            error_message = f\"An error occurred: {e}\"\n            self.status = error_message\n            raise ValueError(error_message) from e\n\n        # An error occurred\n        raise ValueError(self.status)\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false},"json_path":{"tool_mode":false,"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"json_path","value":"","display_name":"JSON File Path","advanced":false,"input_types":["Message"],"dynamic":false,"info":"Provide the path to the JSON file as pure text","title_case":false,"type":"str","_input_type":"MessageTextInput"},"json_string":{"tool_mode":false,"trace_as_input":true,"multiline":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"json_string","value":"","display_name":"JSON String","advanced":false,"input_types":["Message"],"dynamic":false,"info":"Enter a valid JSON string (object or array) to convert to a Data object or list of Data objects","title_case":false,"type":"str","_input_type":"MultilineInput"}},"description":"Convert a JSON file, JSON from a file path, or a JSON string to a Data object or a list of Data objects","icon":"braces","base_classes":["Data"],"display_name":"JSON to Data","documentation":"","custom_fields":{},"output_types":[],"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Data"],"selected":"Data","name":"data","display_name":"Data","method":"convert_json_to_data","value":"__UNDEFINED__","cache":true}],"field_order":["json_file","json_path","json_string"],"beta":false,"legacy":true,"edited":false,"metadata":{},"tool_mode":false},"id":"JSONtoData-Ta1dv","description":"Convert a JSON file, JSON from a file path, or a JSON string to a Data object or a list of Data objects","display_name":"JSON to Data"},"selected":false,"width":320,"height":443,"positionAbsolute":{"x":3329.4763311498004,"y":536.0354101674794},"dragging":false},{"id":"TextInput-pSGqL","type":"genericNode","position":{"x":1575.0703297500336,"y":233.43140760967972},"data":{"node":{"template":{"_type":"Component","code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"from langflow.base.io.text import TextComponent\nfrom langflow.io import MultilineInput, Output\nfrom langflow.schema.message import Message\n\n\nclass TextInputComponent(TextComponent):\n    display_name = \"Text Input\"\n    description = \"Get text inputs from the Playground.\"\n    icon = \"type\"\n    name = \"TextInput\"\n\n    inputs = [\n        MultilineInput(\n            name=\"input_value\",\n            display_name=\"Text\",\n            info=\"Text to be passed as input.\",\n        ),\n    ]\n    outputs = [\n        Output(display_name=\"Text\", name=\"text\", method=\"text_response\"),\n    ]\n\n    def text_response(self) -> Message:\n        return Message(\n            text=self.input_value,\n        )\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false},"input_value":{"tool_mode":false,"trace_as_input":true,"multiline":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"input_value","value":"Here is a complete DSL specification for JsonLogic, including all supported operations and examples for each:\n\n### Accessing Data\n\n- **var**: Retrieve data from the provided data object.\n\n  ```json\n  { \"var\": [\"a\"] }\n  ```\n\n  Data: `{ \"a\": 1, \"b\": 2 }`\n  Result: `1`\n\n- **missing**: Returns an array of any keys that are missing from the data object.\n\n  ```json\n  { \"missing\": [\"a\", \"b\"] }\n  ```\n\n  Data: `{ \"a\": \"apple\", \"c\": \"carrot\" }`\n  Result: `[\"b\"]`\n\n- **missing_some**: Returns an empty array if the minimum number of keys is met, or an array of the missing keys otherwise.\n  ```json\n  { \"missing_some\": [1, [\"a\", \"b\", \"c\"]] }\n  ```\n  Data: `{ \"a\": \"apple\" }`\n  Result: `[]`\n\n### Logic and Boolean Operations\n\n- **if**: Conditional logic.\n\n  ```json\n  { \"if\": [true, \"yes\", \"no\"] }\n  ```\n\n  Result: `\"yes\"`\n\n- **==**: Tests equality, with type coercion.\n\n  ```json\n  { \"==\": [1, \"1\"] }\n  ```\n\n  Result: `true`\n\n- **===**: Tests strict equality.\n\n  ```json\n  { \"===\": [1, \"1\"] }\n  ```\n\n  Result: `false`\n\n- **!=**: Tests not-equal, with type coercion.\n\n  ```json\n  { \"!=\": [1, 2] }\n  ```\n\n  Result: `true`\n\n- **!==**: Tests strict not-equal.\n\n  ```json\n  { \"!==\": [1, \"1\"] }\n  ```\n\n  Result: `true`\n\n- **!**: Logical negation.\n\n  ```json\n  { \"!\": true }\n  ```\n\n  Result: `false`\n\n- **!!**: Double negation, or “cast to a boolean.”\n\n  ```json\n  { \"!!\": [\"0\"] }\n  ```\n\n  Result: `true`\n\n- **or**: Returns the first truthy argument, or the last argument.\n\n  ```json\n  { \"or\": [false, \"a\"] }\n  ```\n\n  Result: `\"a\"`\n\n- **and**: Returns the first falsy argument, or the last argument.\n  ```json\n  { \"and\": [true, \"a\", 3] }\n  ```\n  Result: `3`\n\n### Numeric Operations\n\n- **>**: Greater than.\n\n  ```json\n  { \">\": [2, 1] }\n  ```\n\n  Result: `true`\n\n- **>=**: Greater than or equal to.\n\n  ```json\n  { \">=\": [1, 1] }\n  ```\n\n  Result: `true`\n\n- **<**: Less than.\n\n  ```json\n  { \"<\": [1, 2] }\n  ```\n\n  Result: `true`\n\n- **<=**: Less than or equal to.\n\n  ```json\n  { \"<=\": [1, 1] }\n  ```\n\n  Result: `true`\n\n- **Between**: Test that one value is between two others.\n\n  ```json\n  { \"<\": [1, 2, 3] }\n  ```\n\n  Result: `true`\n\n- **max**: Return the maximum from a list of values.\n\n  ```json\n  { \"max\": [1, 2, 3] }\n  ```\n\n  Result: `3`\n\n- **min**: Return the minimum from a list of values.\n\n  ```json\n  { \"min\": [1, 2, 3] }\n  ```\n\n  Result: `1`\n\n- **+**: Addition.\n\n  ```json\n  { \"+\": [4, 2] }\n  ```\n\n  Result: `6`\n\n- **-**: Subtraction.\n\n  ```json\n  { \"-\": [4, 2] }\n  ```\n\n  Result: `2`\n\n- **\\***: Multiplication.\n\n  ```json\n  { \"*\": [4, 2] }\n  ```\n\n  Result: `8`\n\n- **/**: Division.\n\n  ```json\n  { \"/\": [4, 2] }\n  ```\n\n  Result: `2`\n\n- **%**: Modulo.\n  ```json\n  { \"%\": [101, 2] }\n  ```\n  Result: `1`\n\n### Array Operations\n\n- **map**: Perform an action on every member of an array.\n\n  ```json\n  { \"map\": [{ \"var\": \"integers\" }, { \"*\": [{ \"var\": \"\" }, 2] }] }\n  ```\n\n  Data: `{ \"integers\": [1, 2, 3, 4, 5] }`\n  Result: `[2, 4, 6, 8, 10]`\n\n- **filter**: Keep only elements of the array that pass a test.\n\n  ```json\n  { \"filter\": [{ \"var\": \"integers\" }, { \"%\": [{ \"var\": \"\" }, 2] }] }\n  ```\n\n  Data: `{ \"integers\": [1, 2, 3, 4, 5] }`\n  Result: `[1, 3, 5]`\n\n- **reduce**: Combine all the elements in an array into a single value.\n\n  ```json\n  { \"reduce\": [{ \"var\": \"integers\" }, { \"+\": [{ \"var\": \"current\" }, { \"var\": \"accumulator\" }] }, 0] }\n  ```\n\n  Data: `{ \"integers\": [1, 2, 3, 4, 5] }`\n  Result: `15`\n\n- **all**: Test if all elements in an array pass a test.\n\n  ```json\n  { \"all\": [[1, 2, 3], { \">\": [{ \"var\": \"\" }, 0] }] }\n  ```\n\n  Result: `true`\n\n- **none**: Test if no elements in an array pass a test.\n\n  ```json\n  { \"none\": [[-3, -2, -1], { \">\": [{ \"var\": \"\" }, 0] }] }\n  ```\n\n  Result: `true`\n\n- **some**: Test if some elements in an array pass a test.\n\n  ```json\n  { \"some\": [[-1, 0, 1], { \">\": [{ \"var\": \"\" }, 0] }] }\n  ```\n\n  Result: `true`\n\n- **merge**: Merge one or more arrays into one array.\n\n  ```json\n  {\n    \"merge\": [\n      [1, 2],\n      [3, 4]\n    ]\n  }\n  ```\n\n  Result: `[1, 2, 3, 4]`\n\n- **in**: Test if the first argument is a member of the array.\n  ```json\n  { \"in\": [\"Ringo\", [\"John\", \"Paul\", \"George\", \"Ringo\"]] }\n  ```\n  Result: `true`\n\n### String Operations\n\n- **in**: Test if the first argument is a substring of the second.\n\n  ```json\n  { \"in\": [\"Spring\", \"Springfield\"] }\n  ```\n\n  Result: `true`\n\n- **cat**: Concatenate all the supplied arguments.\n\n  ```json\n  { \"cat\": [\"I love\", \" pie\"] }\n  ```\n\n  Result: `\"I love pie\"`\n\n- **substr**: Get a portion of a string.\n  ```json\n  { \"substr\": [\"jsonlogic\", 4] }\n  ```\n  Result: `\"logic\"`\n\n### Miscellaneous\n\n- **log**: Logs the first value to console, then passes it through unmodified.\n  ```json\n  { \"log\": \"apple\" }\n  ```\n  Result: `\"apple\"` (Check your developer console!)\n\nThis specification provides a comprehensive overview of the operations supported by JsonLogic, along with examples to illustrate their usage.\n","display_name":"Text","advanced":false,"input_types":["Message"],"dynamic":false,"info":"Text to be passed as input.","title_case":false,"type":"str","_input_type":"MultilineInput"}},"description":"Get text inputs from the Playground.","icon":"type","base_classes":["Message"],"display_name":"JsonLogic DSL Spec","documentation":"","custom_fields":{},"output_types":[],"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Message"],"selected":"Message","name":"text","display_name":"Text","method":"text_response","value":"__UNDEFINED__","cache":true}],"field_order":["input_value"],"beta":false,"legacy":false,"edited":false,"metadata":{},"tool_mode":false,"lf_version":"1.1.0"},"type":"TextInput","id":"TextInput-pSGqL"},"selected":false,"width":320,"height":233,"positionAbsolute":{"x":1575.0703297500336,"y":233.43140760967972},"dragging":false},{"id":"LMStudioModel-3aSse","type":"genericNode","position":{"x":2482.885945849777,"y":1219.5054259289439},"data":{"node":{"template":{"_type":"Component","output_parser":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"output_parser","value":"","display_name":"Output Parser","advanced":true,"input_types":["OutputParser"],"dynamic":false,"info":"The parser to use to parse the output of the model","title_case":false,"type":"other","_input_type":"HandleInput"},"api_key":{"load_from_db":false,"required":false,"placeholder":"","show":true,"name":"api_key","value":"","display_name":"LM Studio API Key","advanced":true,"input_types":["Message"],"dynamic":false,"info":"The LM Studio API Key to use for LM Studio.","title_case":false,"password":true,"type":"str","_input_type":"SecretStrInput"},"base_url":{"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"base_url","value":"http://localhost:1234/v1","display_name":"Base URL","advanced":false,"dynamic":false,"info":"Endpoint of the LM Studio API. Defaults to 'http://localhost:1234/v1' if not specified.","title_case":false,"type":"str","_input_type":"StrInput"},"code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"from typing import Any\nfrom urllib.parse import urljoin\n\nimport httpx\nfrom langchain_openai import ChatOpenAI\nfrom pydantic.v1 import SecretStr\nfrom typing_extensions import override\n\nfrom langflow.base.models.model import LCModelComponent\nfrom langflow.field_typing import LanguageModel\nfrom langflow.field_typing.range_spec import RangeSpec\nfrom langflow.inputs import DictInput, DropdownInput, FloatInput, IntInput, SecretStrInput, StrInput\nfrom langflow.inputs.inputs import HandleInput\n\n\nclass LMStudioModelComponent(LCModelComponent):\n    display_name = \"LM Studio\"\n    description = \"Generate text using LM Studio Local LLMs.\"\n    icon = \"LMStudio\"\n    name = \"LMStudioModel\"\n\n    @override\n    def update_build_config(self, build_config: dict, field_value: Any, field_name: str | None = None):\n        if field_name == \"model_name\":\n            base_url_dict = build_config.get(\"base_url\", {})\n            base_url_load_from_db = base_url_dict.get(\"load_from_db\", False)\n            base_url_value = base_url_dict.get(\"value\")\n            if base_url_load_from_db:\n                base_url_value = self.variables(base_url_value)\n            elif not base_url_value:\n                base_url_value = \"http://localhost:1234/v1\"\n            build_config[\"model_name\"][\"options\"] = self.get_model(base_url_value)\n\n        return build_config\n\n    def get_model(self, base_url_value: str) -> list[str]:\n        try:\n            url = urljoin(base_url_value, \"/v1/models\")\n            with httpx.Client() as client:\n                response = client.get(url)\n                response.raise_for_status()\n                data = response.json()\n\n                return [model[\"id\"] for model in data.get(\"data\", [])]\n        except Exception as e:\n            msg = \"Could not retrieve models. Please, make sure the LM Studio server is running.\"\n            raise ValueError(msg) from e\n\n    inputs = [\n        *LCModelComponent._base_inputs,\n        IntInput(\n            name=\"max_tokens\",\n            display_name=\"Max Tokens\",\n            advanced=True,\n            info=\"The maximum number of tokens to generate. Set to 0 for unlimited tokens.\",\n            range_spec=RangeSpec(min=0, max=128000),\n        ),\n        DictInput(name=\"model_kwargs\", display_name=\"Model Kwargs\", advanced=True),\n        DropdownInput(\n            name=\"model_name\",\n            display_name=\"Model Name\",\n            advanced=False,\n            refresh_button=True,\n        ),\n        StrInput(\n            name=\"base_url\",\n            display_name=\"Base URL\",\n            advanced=False,\n            info=\"Endpoint of the LM Studio API. Defaults to 'http://localhost:1234/v1' if not specified.\",\n            value=\"http://localhost:1234/v1\",\n        ),\n        SecretStrInput(\n            name=\"api_key\",\n            display_name=\"LM Studio API Key\",\n            info=\"The LM Studio API Key to use for LM Studio.\",\n            advanced=True,\n            value=\"LMSTUDIO_API_KEY\",\n        ),\n        FloatInput(name=\"temperature\", display_name=\"Temperature\", value=0.1),\n        IntInput(\n            name=\"seed\",\n            display_name=\"Seed\",\n            info=\"The seed controls the reproducibility of the job.\",\n            advanced=True,\n            value=1,\n        ),\n        HandleInput(\n            name=\"output_parser\",\n            display_name=\"Output Parser\",\n            info=\"The parser to use to parse the output of the model\",\n            advanced=True,\n            input_types=[\"OutputParser\"],\n        ),\n    ]\n\n    def build_model(self) -> LanguageModel:  # type: ignore[type-var]\n        lmstudio_api_key = self.api_key\n        temperature = self.temperature\n        model_name: str = self.model_name\n        max_tokens = self.max_tokens\n        model_kwargs = self.model_kwargs or {}\n        base_url = self.base_url or \"http://localhost:1234/v1\"\n        seed = self.seed\n\n        api_key = SecretStr(lmstudio_api_key) if lmstudio_api_key else None\n\n        return ChatOpenAI(\n            max_tokens=max_tokens or None,\n            model_kwargs=model_kwargs,\n            model=model_name,\n            base_url=base_url,\n            api_key=api_key,\n            temperature=temperature if temperature is not None else 0.1,\n            seed=seed,\n        )\n\n    def _get_exception_message(self, e: Exception):\n        \"\"\"Get a message from an LM Studio exception.\n\n        Args:\n            e (Exception): The exception to get the message from.\n\n        Returns:\n            str: The message from the exception.\n        \"\"\"\n        try:\n            from openai import BadRequestError\n        except ImportError:\n            return None\n        if isinstance(e, BadRequestError):\n            message = e.body.get(\"message\")\n            if message:\n                return message\n        return None\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false},"input_value":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"input_value","value":"","display_name":"Input","advanced":false,"input_types":["Message"],"dynamic":false,"info":"","title_case":false,"type":"str","_input_type":"MessageInput"},"max_tokens":{"trace_as_metadata":true,"range_spec":{"step_type":"float","min":0,"max":128000,"step":0.1},"list":false,"required":false,"placeholder":"","show":true,"name":"max_tokens","value":"","display_name":"Max Tokens","advanced":true,"dynamic":false,"info":"The maximum number of tokens to generate. Set to 0 for unlimited tokens.","title_case":false,"type":"int","_input_type":"IntInput"},"model_kwargs":{"trace_as_input":true,"list":false,"required":false,"placeholder":"","show":true,"name":"model_kwargs","value":{},"display_name":"Model Kwargs","advanced":true,"dynamic":false,"info":"","title_case":false,"type":"dict","_input_type":"DictInput"},"model_name":{"tool_mode":false,"trace_as_metadata":true,"options":[],"combobox":false,"required":false,"placeholder":"","show":true,"name":"model_name","value":"","display_name":"Model Name","advanced":false,"dynamic":false,"info":"","refresh_button":true,"title_case":false,"type":"str","_input_type":"DropdownInput"},"seed":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"seed","value":1,"display_name":"Seed","advanced":true,"dynamic":false,"info":"The seed controls the reproducibility of the job.","title_case":false,"type":"int","_input_type":"IntInput"},"stream":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"stream","value":false,"display_name":"Stream","advanced":false,"dynamic":false,"info":"Stream the response from the model. Streaming works only in Chat.","title_case":false,"type":"bool","_input_type":"BoolInput"},"system_message":{"tool_mode":false,"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"system_message","value":"","display_name":"System Message","advanced":false,"input_types":["Message"],"dynamic":false,"info":"System message to pass to the model.","title_case":false,"type":"str","_input_type":"MessageTextInput"},"temperature":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"temperature","value":0,"display_name":"Temperature","advanced":false,"dynamic":false,"info":"","title_case":false,"type":"float","_input_type":"FloatInput"}},"description":"Generate text using LM Studio Local LLMs.","icon":"LMStudio","base_classes":["LanguageModel","Message"],"display_name":"LM Studio","documentation":"","custom_fields":{},"output_types":[],"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Message"],"selected":"Message","name":"text_output","display_name":"Text","method":"text_response","value":"__UNDEFINED__","cache":true,"required_inputs":[]},{"types":["LanguageModel"],"selected":"LanguageModel","name":"model_output","display_name":"Language Model","method":"build_model","value":"__UNDEFINED__","cache":true,"required_inputs":[]}],"field_order":["input_value","system_message","stream","max_tokens","model_kwargs","model_name","base_url","api_key","temperature","seed","output_parser"],"beta":false,"legacy":false,"edited":false,"metadata":{},"tool_mode":false},"type":"LMStudioModel","id":"LMStudioModel-3aSse"},"selected":false,"width":320,"height":679},{"id":"JSONtoData-OoArN","type":"genericNode","position":{"x":2975.8400895768073,"y":1396.0744299974406},"data":{"type":"JSONtoData","node":{"template":{"_type":"Component","json_file":{"trace_as_metadata":true,"file_path":"","fileTypes":["json"],"list":false,"required":false,"placeholder":"","show":true,"name":"json_file","value":"","display_name":"JSON File","advanced":false,"dynamic":false,"info":"Upload a JSON file to convert to a Data object or list of Data objects","title_case":false,"type":"file","_input_type":"FileInput"},"code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"import json\nfrom pathlib import Path\n\nfrom json_repair import repair_json\n\nfrom langflow.custom import Component\nfrom langflow.io import FileInput, MessageTextInput, MultilineInput, Output\nfrom langflow.schema import Data\n\n\nclass JSONToDataComponent(Component):\n    display_name = \"Load JSON\"\n    description = (\n        \"Convert a JSON file, JSON from a file path, or a JSON string to a Data object or a list of Data objects\"\n    )\n    icon = \"braces\"\n    name = \"JSONtoData\"\n    legacy = True\n\n    inputs = [\n        FileInput(\n            name=\"json_file\",\n            display_name=\"JSON File\",\n            file_types=[\"json\"],\n            info=\"Upload a JSON file to convert to a Data object or list of Data objects\",\n        ),\n        MessageTextInput(\n            name=\"json_path\",\n            display_name=\"JSON File Path\",\n            info=\"Provide the path to the JSON file as pure text\",\n        ),\n        MultilineInput(\n            name=\"json_string\",\n            display_name=\"JSON String\",\n            info=\"Enter a valid JSON string (object or array) to convert to a Data object or list of Data objects\",\n        ),\n    ]\n\n    outputs = [\n        Output(name=\"data\", display_name=\"Data\", method=\"convert_json_to_data\"),\n    ]\n\n    def convert_json_to_data(self) -> Data | list[Data]:\n        if sum(bool(field) for field in [self.json_file, self.json_path, self.json_string]) != 1:\n            msg = \"Please provide exactly one of: JSON file, file path, or JSON string.\"\n            self.status = msg\n            raise ValueError(msg)\n\n        json_data = None\n\n        try:\n            if self.json_file:\n                resolved_path = self.resolve_path(self.json_file)\n                file_path = Path(resolved_path)\n                if file_path.suffix.lower() != \".json\":\n                    self.status = \"The provided file must be a JSON file.\"\n                else:\n                    json_data = file_path.read_text(encoding=\"utf-8\")\n\n            elif self.json_path:\n                file_path = Path(self.json_path)\n                if file_path.suffix.lower() != \".json\":\n                    self.status = \"The provided file must be a JSON file.\"\n                else:\n                    json_data = file_path.read_text(encoding=\"utf-8\")\n\n            else:\n                json_data = self.json_string\n\n            if json_data:\n                # Try to parse the JSON string\n                try:\n                    parsed_data = json.loads(json_data)\n                except json.JSONDecodeError:\n                    # If JSON parsing fails, try to repair the JSON string\n                    repaired_json_string = repair_json(json_data)\n                    parsed_data = json.loads(repaired_json_string)\n\n                # Check if the parsed data is a list\n                if isinstance(parsed_data, list):\n                    result = [Data(data=item) for item in parsed_data]\n                else:\n                    result = Data(data=parsed_data)\n                self.status = result\n                return result\n\n        except (json.JSONDecodeError, SyntaxError, ValueError) as e:\n            error_message = f\"Invalid JSON or Python literal: {e}\"\n            self.status = error_message\n            raise ValueError(error_message) from e\n\n        except Exception as e:\n            error_message = f\"An error occurred: {e}\"\n            self.status = error_message\n            raise ValueError(error_message) from e\n\n        # An error occurred\n        raise ValueError(self.status)\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false},"json_path":{"tool_mode":false,"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"json_path","value":"","display_name":"JSON File Path","advanced":false,"input_types":["Message"],"dynamic":false,"info":"Provide the path to the JSON file as pure text","title_case":false,"type":"str","_input_type":"MessageTextInput"},"json_string":{"tool_mode":false,"trace_as_input":true,"multiline":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"json_string","value":"","display_name":"JSON String","advanced":false,"input_types":["Message"],"dynamic":false,"info":"Enter a valid JSON string (object or array) to convert to a Data object or list of Data objects","title_case":false,"type":"str","_input_type":"MultilineInput"}},"description":"Convert a JSON file, JSON from a file path, or a JSON string to a Data object or a list of Data objects","icon":"braces","base_classes":["Data"],"display_name":"JSON to Data","documentation":"","custom_fields":{},"output_types":[],"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Data"],"selected":"Data","name":"data","display_name":"Data","method":"convert_json_to_data","value":"__UNDEFINED__","cache":true}],"field_order":["json_file","json_path","json_string"],"beta":false,"legacy":true,"edited":false,"metadata":{},"tool_mode":false},"id":"JSONtoData-OoArN","description":"Convert a JSON file, JSON from a file path, or a JSON string to a Data object or a list of Data objects","display_name":"JSON to Data"},"selected":false,"width":320,"height":443,"positionAbsolute":{"x":2975.8400895768073,"y":1396.0744299974406},"dragging":false}],"edges":[{"source":"URL-6WzwK","sourceHandle":"{œdataTypeœ:œURLœ,œidœ:œURL-6WzwKœ,œnameœ:œtextœ,œoutput_typesœ:[œMessageœ]}","target":"Prompt-7Pxkb","targetHandle":"{œfieldNameœ:œjsonLogicœ,œidœ:œPrompt-7Pxkbœ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}","data":{"targetHandle":{"fieldName":"jsonLogic","id":"Prompt-7Pxkb","inputTypes":["Message","Text"],"type":"str"},"sourceHandle":{"dataType":"URL","id":"URL-6WzwK","name":"text","output_types":["Message"]}},"id":"reactflow__edge-URL-6WzwK{œdataTypeœ:œURLœ,œidœ:œURL-6WzwKœ,œnameœ:œtextœ,œoutput_typesœ:[œMessageœ]}-Prompt-7Pxkb{œfieldNameœ:œjsonLogicœ,œidœ:œPrompt-7Pxkbœ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}","animated":false,"className":""},{"source":"Prompt-7Pxkb","sourceHandle":"{œdataTypeœ:œPromptœ,œidœ:œPrompt-7Pxkbœ,œnameœ:œpromptœ,œoutput_typesœ:[œMessageœ]}","target":"OpenAIModel-mexlT","targetHandle":"{œfieldNameœ:œinput_valueœ,œidœ:œOpenAIModel-mexlTœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}","data":{"targetHandle":{"fieldName":"input_value","id":"OpenAIModel-mexlT","inputTypes":["Message"],"type":"str"},"sourceHandle":{"dataType":"Prompt","id":"Prompt-7Pxkb","name":"prompt","output_types":["Message"]}},"id":"reactflow__edge-Prompt-7Pxkb{œdataTypeœ:œPromptœ,œidœ:œPrompt-7Pxkbœ,œnameœ:œpromptœ,œoutput_typesœ:[œMessageœ]}-OpenAIModel-mexlT{œfieldNameœ:œinput_valueœ,œidœ:œOpenAIModel-mexlTœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}","animated":false,"className":""},{"source":"URL-6WzwK","sourceHandle":"{œdataTypeœ:œURLœ,œidœ:œURL-6WzwKœ,œnameœ:œtextœ,œoutput_typesœ:[œMessageœ]}","target":"Prompt-aIaC7","targetHandle":"{œfieldNameœ:œjsonLogicœ,œidœ:œPrompt-aIaC7œ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}","data":{"targetHandle":{"fieldName":"jsonLogic","id":"Prompt-aIaC7","inputTypes":["Message","Text"],"type":"str"},"sourceHandle":{"dataType":"URL","id":"URL-6WzwK","name":"text","output_types":["Message"]}},"id":"reactflow__edge-URL-6WzwK{œdataTypeœ:œURLœ,œidœ:œURL-6WzwKœ,œnameœ:œtextœ,œoutput_typesœ:[œMessageœ]}-Prompt-aIaC7{œfieldNameœ:œjsonLogicœ,œidœ:œPrompt-aIaC7œ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}","animated":false,"className":""},{"source":"OpenAIModel-mexlT","sourceHandle":"{œdataTypeœ:œOpenAIModelœ,œidœ:œOpenAIModel-mexlTœ,œnameœ:œtext_outputœ,œoutput_typesœ:[œMessageœ]}","target":"Prompt-aIaC7","targetHandle":"{œfieldNameœ:œjsonLogicSpecœ,œidœ:œPrompt-aIaC7œ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}","data":{"targetHandle":{"fieldName":"jsonLogicSpec","id":"Prompt-aIaC7","inputTypes":["Message","Text"],"type":"str"},"sourceHandle":{"dataType":"OpenAIModel","id":"OpenAIModel-mexlT","name":"text_output","output_types":["Message"]}},"id":"reactflow__edge-OpenAIModel-mexlT{œdataTypeœ:œOpenAIModelœ,œidœ:œOpenAIModel-mexlTœ,œnameœ:œtext_outputœ,œoutput_typesœ:[œMessageœ]}-Prompt-aIaC7{œfieldNameœ:œjsonLogicSpecœ,œidœ:œPrompt-aIaC7œ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}","animated":false,"className":""},{"source":"Prompt-aIaC7","sourceHandle":"{œdataTypeœ:œPromptœ,œidœ:œPrompt-aIaC7œ,œnameœ:œpromptœ,œoutput_typesœ:[œMessageœ]}","target":"OpenAIModel-Aofzl","targetHandle":"{œfieldNameœ:œinput_valueœ,œidœ:œOpenAIModel-Aofzlœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}","data":{"targetHandle":{"fieldName":"input_value","id":"OpenAIModel-Aofzl","inputTypes":["Message"],"type":"str"},"sourceHandle":{"dataType":"Prompt","id":"Prompt-aIaC7","name":"prompt","output_types":["Message"]}},"id":"reactflow__edge-Prompt-aIaC7{œdataTypeœ:œPromptœ,œidœ:œPrompt-aIaC7œ,œnameœ:œpromptœ,œoutput_typesœ:[œMessageœ]}-OpenAIModel-Aofzl{œfieldNameœ:œinput_valueœ,œidœ:œOpenAIModel-Aofzlœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}","animated":false,"className":""},{"source":"TextInput-i0IHc","sourceHandle":"{œdataTypeœ:œTextInputœ,œidœ:œTextInput-i0IHcœ,œnameœ:œtextœ,œoutput_typesœ:[œMessageœ]}","target":"Prompt-8jxiq","targetHandle":"{œfieldNameœ:œlawœ,œidœ:œPrompt-8jxiqœ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}","data":{"targetHandle":{"fieldName":"law","id":"Prompt-8jxiq","inputTypes":["Message","Text"],"type":"str"},"sourceHandle":{"dataType":"TextInput","id":"TextInput-i0IHc","name":"text","output_types":["Message"]}},"id":"reactflow__edge-TextInput-i0IHc{œdataTypeœ:œTextInputœ,œidœ:œTextInput-i0IHcœ,œnameœ:œtextœ,œoutput_typesœ:[œMessageœ]}-Prompt-8jxiq{œfieldNameœ:œlawœ,œidœ:œPrompt-8jxiqœ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}","animated":false,"className":""},{"source":"Prompt-8jxiq","sourceHandle":"{œdataTypeœ:œPromptœ,œidœ:œPrompt-8jxiqœ,œnameœ:œpromptœ,œoutput_typesœ:[œMessageœ]}","target":"OpenAIModel-CQetX","targetHandle":"{œfieldNameœ:œinput_valueœ,œidœ:œOpenAIModel-CQetXœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}","data":{"targetHandle":{"fieldName":"input_value","id":"OpenAIModel-CQetX","inputTypes":["Message"],"type":"str"},"sourceHandle":{"dataType":"Prompt","id":"Prompt-8jxiq","name":"prompt","output_types":["Message"]}},"id":"reactflow__edge-Prompt-8jxiq{œdataTypeœ:œPromptœ,œidœ:œPrompt-8jxiqœ,œnameœ:œpromptœ,œoutput_typesœ:[œMessageœ]}-OpenAIModel-CQetX{œfieldNameœ:œinput_valueœ,œidœ:œOpenAIModel-CQetXœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}","animated":false,"className":""},{"source":"OpenAIModel-CQetX","sourceHandle":"{œdataTypeœ:œOpenAIModelœ,œidœ:œOpenAIModel-CQetXœ,œnameœ:œtext_outputœ,œoutput_typesœ:[œMessageœ]}","target":"JSONtoData-Ta1dv","targetHandle":"{œfieldNameœ:œjson_stringœ,œidœ:œJSONtoData-Ta1dvœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}","data":{"targetHandle":{"fieldName":"json_string","id":"JSONtoData-Ta1dv","inputTypes":["Message"],"type":"str"},"sourceHandle":{"dataType":"OpenAIModel","id":"OpenAIModel-CQetX","name":"text_output","output_types":["Message"]}},"id":"reactflow__edge-OpenAIModel-CQetX{œdataTypeœ:œOpenAIModelœ,œidœ:œOpenAIModel-CQetXœ,œnameœ:œtext_outputœ,œoutput_typesœ:[œMessageœ]}-JSONtoData-Ta1dv{œfieldNameœ:œjson_stringœ,œidœ:œJSONtoData-Ta1dvœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}","animated":false,"className":""},{"source":"TextInput-pSGqL","sourceHandle":"{œdataTypeœ:œTextInputœ,œidœ:œTextInput-pSGqLœ,œnameœ:œtextœ,œoutput_typesœ:[œMessageœ]}","target":"Prompt-8jxiq","targetHandle":"{œfieldNameœ:œjsonLogicœ,œidœ:œPrompt-8jxiqœ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}","data":{"targetHandle":{"fieldName":"jsonLogic","id":"Prompt-8jxiq","inputTypes":["Message","Text"],"type":"str"},"sourceHandle":{"dataType":"TextInput","id":"TextInput-pSGqL","name":"text","output_types":["Message"]}},"id":"reactflow__edge-TextInput-pSGqL{œdataTypeœ:œTextInputœ,œidœ:œTextInput-pSGqLœ,œnameœ:œtextœ,œoutput_typesœ:[œMessageœ]}-Prompt-8jxiq{œfieldNameœ:œjsonLogicœ,œidœ:œPrompt-8jxiqœ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}","className":"","animated":false},{"source":"Prompt-8jxiq","sourceHandle":"{œdataTypeœ:œPromptœ,œidœ:œPrompt-8jxiqœ,œnameœ:œpromptœ,œoutput_typesœ:[œMessageœ]}","target":"LMStudioModel-3aSse","targetHandle":"{œfieldNameœ:œinput_valueœ,œidœ:œLMStudioModel-3aSseœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}","data":{"targetHandle":{"fieldName":"input_value","id":"LMStudioModel-3aSse","inputTypes":["Message"],"type":"str"},"sourceHandle":{"dataType":"Prompt","id":"Prompt-8jxiq","name":"prompt","output_types":["Message"]}},"id":"reactflow__edge-Prompt-8jxiq{œdataTypeœ:œPromptœ,œidœ:œPrompt-8jxiqœ,œnameœ:œpromptœ,œoutput_typesœ:[œMessageœ]}-LMStudioModel-3aSse{œfieldNameœ:œinput_valueœ,œidœ:œLMStudioModel-3aSseœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}","animated":false,"className":""},{"source":"LMStudioModel-3aSse","sourceHandle":"{œdataTypeœ:œLMStudioModelœ,œidœ:œLMStudioModel-3aSseœ,œnameœ:œtext_outputœ,œoutput_typesœ:[œMessageœ]}","target":"JSONtoData-OoArN","targetHandle":"{œfieldNameœ:œjson_stringœ,œidœ:œJSONtoData-OoArNœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}","data":{"targetHandle":{"fieldName":"json_string","id":"JSONtoData-OoArN","inputTypes":["Message"],"type":"str"},"sourceHandle":{"dataType":"LMStudioModel","id":"LMStudioModel-3aSse","name":"text_output","output_types":["Message"]}},"id":"reactflow__edge-LMStudioModel-3aSse{œdataTypeœ:œLMStudioModelœ,œidœ:œLMStudioModel-3aSseœ,œnameœ:œtext_outputœ,œoutput_typesœ:[œMessageœ]}-JSONtoData-OoArN{œfieldNameœ:œjson_stringœ,œidœ:œJSONtoData-OoArNœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}","animated":false,"className":""}],"viewport":{"x":172.04832535603305,"y":194.24825157528306,"zoom":0.47230857004302834}},"is_component":false,"updated_at":"2024-11-17T00:41:56+00:00","description":"Convert a law to JsonLogic","icon_bg_color":null,"webhook":false,"icon":null}